#!/usr/bin/env python3
"""
æ•°æ®åº“ä¼˜åŒ–åˆ†æè„šæœ¬
åˆ†ææ•°æ®åº“æ¨¡å‹è®¾è®¡å’ŒæŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–å»ºè®®
"""

import os
import re
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple

class DatabaseOptimizationAnalyzer:
    """æ•°æ®åº“ä¼˜åŒ–åˆ†æå™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.backend_root = self.project_root / "backend"
        self.report = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(project_root),
            "analysis": {},
            "recommendations": [],
            "optimization_score": {}
        }

    def analyze_database_design(self):
        """åˆ†ææ•°æ®åº“è®¾è®¡"""
        print("ğŸ—„ï¸ åˆ†ææ•°æ®åº“è®¾è®¡...")

        db_analysis = {
            "models": {},
            "relationships": [],
            "indexes": [],
            "queries": [],
            "optimization_opportunities": []
        }

        try:
            models_dir = self.backend_root / "src" / "models"
            if not models_dir.exists():
                print("æœªæ‰¾åˆ°modelsç›®å½•")
                return

            model_files = list(models_dir.glob("*.py"))
            for model_file in model_files:
                if model_file.name == "__init__.py":
                    continue

                model_info = self._analyze_model_file(model_file)
                if model_info:
                    db_analysis["models"][model_file.stem] = model_info

            # åˆ†æå…³ç³»å’Œç´¢å¼•
            self._analyze_relationships(db_analysis)
            self._analyze_indexes(db_analysis)
            self._analyze_query_patterns(db_analysis)
            self._generate_optimization_suggestions(db_analysis)

        except Exception as e:
            print(f"æ•°æ®åº“è®¾è®¡åˆ†æå¤±è´¥: {e}")

        self.report["analysis"]["database_design"] = db_analysis

    def _analyze_model_file(self, model_file: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ¨¡å‹æ–‡ä»¶"""
        try:
            with open(model_file, 'r', encoding='utf-8') as f:
                content = f.read()

            model_info = {
                "classes": [],
                "fields": [],
                "relationships": [],
                "indexes": [],
                "constraints": [],
                "table_names": []
            }

            # æŸ¥æ‰¾è¡¨å
            table_pattern = r'__tablename__\s*=\s*["\']([^"\']+)["\']'
            table_matches = re.findall(table_pattern, content)
            model_info["table_names"] = table_matches

            # æŸ¥æ‰¾ç±»å®šä¹‰
            class_pattern = r'class\s+(\w+)\s*\([^)]*\):'
            class_matches = re.findall(class_pattern, content)
            model_info["classes"] = class_matches

            # æŸ¥æ‰¾å­—æ®µå®šä¹‰
            field_patterns = [
                r'(\w+)\s*=\s*Column\([^)]+\)',
                r'(\w+)\s*=\s*(String|Text|Integer|DateTime|DECIMAL|Boolean|JSON)\([^)]*\)'
            ]

            for pattern in field_patterns:
                field_matches = re.findall(pattern, content, re.MULTILINE)
                for match in field_matches:
                    if isinstance(match, tuple):
                        field_name = match[0]
                    else:
                        field_name = match
                    if field_name not in ['id', 'created_at', 'updated_at']:
                        model_info["fields"].append(field_name)

            # æŸ¥æ‰¾å…³ç³»å®šä¹‰
            relationship_pattern = r'(\w+)\s*=\s*relationship\([^)]+\)'
            relationship_matches = re.findall(relationship_pattern, content)
            model_info["relationships"] = relationship_matches

            # æŸ¥æ‰¾ç´¢å¼•å®šä¹‰
            index_patterns = [
                r'Index\([^)]+\)',
                r'__table_args__\s*=\s*\([^)]*Index[^)]*\)',
                r'@index_property\([^)]+\)'
            ]

            for pattern in index_patterns:
                index_matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                model_info["indexes"].extend(index_matches)

            # æŸ¥æ‰¾çº¦æŸ
            constraint_patterns = [
                r'unique\s*=\s*True',
                r'nullable\s*=\s*False',
                r'foreign_key\s*=\s*\w+',
                r'primary_key\s*=\s*True'
            ]

            for pattern in constraint_patterns:
                constraint_matches = re.findall(pattern, content)
                model_info["constraints"].extend(constraint_matches)

            return model_info

        except Exception as e:
            print(f"åˆ†ææ¨¡å‹æ–‡ä»¶å¤±è´¥ {model_file}: {e}")
            return None

    def _analyze_relationships(self, db_analysis: Dict[str, Any]):
        """åˆ†æè¡¨å…³ç³»"""
        print("ğŸ”— åˆ†æè¡¨å…³ç³»...")

        relationships = []
        for model_name, model_info in db_analysis["models"].items():
            for rel in model_info["relationships"]:
                relationships.append({
                    "source_model": model_name,
                    "relationship_name": rel,
                    "type": "relationship"
                })

        db_analysis["relationships"] = relationships

    def _analyze_indexes(self, db_analysis: Dict[str, Any]):
        """åˆ†æç´¢å¼•ä½¿ç”¨"""
        print("ğŸ“Š åˆ†æç´¢å¼•ä½¿ç”¨...")

        indexes = []
        index_coverage = {}

        for model_name, model_info in db_analysis["models"].items():
            model_indexes = model_info["indexes"]
            fields = model_info["fields"]
            table_names = model_info["table_names"]

            # è®¡ç®—ç´¢å¼•è¦†ç›–ç‡
            indexed_fields = set()
            for index in model_indexes:
                # ä»ç´¢å¼•å®šä¹‰ä¸­æå–å­—æ®µå
                field_matches = re.findall(r'["\']([^"\']+)["\']', index)
                indexed_fields.update(field_matches)

            coverage = len(indexed_fields) / max(len(fields), 1) * 100 if fields else 0
            index_coverage[model_name] = {
                "total_fields": len(fields),
                "indexed_fields": len(indexed_fields),
                "coverage_percent": round(coverage, 1),
                "indexes": model_indexes,
                "missing_indexes": list(set(fields) - indexed_fields)
            }

            indexes.append({
                "model": model_name,
                "table": table_names[0] if table_names else model_name.lower(),
                "index_count": len(model_indexes),
                "coverage": coverage
            })

        db_analysis["indexes"] = indexes
        db_analysis["index_coverage"] = index_coverage

    def _analyze_query_patterns(self, db_analysis: Dict[str, Any]):
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        print("ğŸ” åˆ†ææŸ¥è¯¢æ¨¡å¼...")

        query_patterns = {
            "frequent_queries": [],
            "potential_slow_queries": [],
            "join_operations": [],
            "filter_operations": []
        }

        # æ‰«ææœåŠ¡æ–‡ä»¶ä¸­çš„æŸ¥è¯¢æ¨¡å¼
        services_dir = self.backend_root / "src" / "services"
        if services_dir.exists():
            service_files = list(services_dir.glob("*.py"))

            for service_file in service_files:
                try:
                    with open(service_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # æŸ¥æ‰¾æŸ¥è¯¢æ¨¡å¼
                    query_patterns["frequent_queries"].extend(
                        self._extract_query_patterns(content, "frequent")
                    )
                    query_patterns["potential_slow_queries"].extend(
                        self._extract_query_patterns(content, "slow")
                    )
                    query_patterns["join_operations"].extend(
                        self._extract_query_patterns(content, "join")
                    )
                    query_patterns["filter_operations"].extend(
                        self._extract_query_patterns(content, "filter")
                    )

                except Exception as e:
                    continue

        db_analysis["query_patterns"] = query_patterns

    def _extract_query_patterns(self, content: str, pattern_type: str) -> List[Dict[str, Any]]:
        """æå–æŸ¥è¯¢æ¨¡å¼"""
        patterns = []

        if pattern_type == "frequent":
            # æŸ¥æ‰¾é¢‘ç¹æŸ¥è¯¢æ¨¡å¼
            frequent_patterns = [
                r'\.query\([^)]+\)\.first\(\)',
                r'\.query\([^)]+\)\.all\(\)',
                r'\.get\([^)]+\)',
                r'\.filter_by\([^)]+\)'
            ]
            for pattern in frequent_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    patterns.append({
                        "pattern": match,
                        "type": "frequent",
                        "file": "service_file"
                    })

        elif pattern_type == "slow":
            # æŸ¥æ‰¾å¯èƒ½çš„æ…¢æŸ¥è¯¢
            slow_patterns = [
                r'\.query\([^)]+\)\.filter\([^)]+\)\.filter\([^)]+\)',  # å¤šé‡è¿‡æ»¤
                r'\.join\([^)]+\)\.join\([^)]+\)',  # å¤šè¡¨è¿æ¥
                r'\.order_by\([^)]+\)\.limit\(\d+\)',  # æ’åºé™åˆ¶
                r'func\.count\(',  # èšåˆå‡½æ•°
            ]
            for pattern in slow_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    patterns.append({
                        "pattern": match,
                        "type": "potential_slow",
                        "file": "service_file"
                    })

        elif pattern_type == "join":
            # æŸ¥æ‰¾è¿æ¥æ“ä½œ
            join_patterns = [
                r'\.join\([^)]+\)',
                r'joinload\(',
                r'selectinload\(',
                r'subqueryload\('
            ]
            for pattern in join_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    patterns.append({
                        "pattern": match,
                        "type": "join",
                        "file": "service_file"
                    })

        elif pattern_type == "filter":
            # æŸ¥æ‰¾è¿‡æ»¤æ“ä½œ
            filter_patterns = [
                r'\.filter\([^)]+\)',
                r'\.filter_by\([^)]+\)',
                r'where\s*=',
                r'and_\(',
                r'or_\('
            ]
            for pattern in filter_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    patterns.append({
                        "pattern": match,
                        "type": "filter",
                        "file": "service_file"
                    })

        return patterns

    def _generate_optimization_suggestions(self, db_analysis: Dict[str, Any]):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("ğŸ’¡ ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–å»ºè®®...")

        suggestions = []

        # åŸºäºç´¢å¼•è¦†ç›–ç‡çš„å»ºè®®
        index_coverage = db_analysis.get("index_coverage", {})
        for model_name, coverage_info in index_coverage.items():
            coverage = coverage_info.get("coverage_percent", 0)
            missing_indexes = coverage_info.get("missing_indexes", [])

            if coverage < 50:
                suggestions.append({
                    "category": "ç´¢å¼•ä¼˜åŒ–",
                    "priority": "é«˜",
                    "model": model_name,
                    "issue": f"ç´¢å¼•è¦†ç›–ç‡è¿‡ä½ ({coverage}%)",
                    "suggestion": f"ä¸ºå­—æ®µæ·»åŠ ç´¢å¼•: {', '.join(missing_indexes[:5])}",
                    "impact": "æ˜¾è‘—æå‡æŸ¥è¯¢æ€§èƒ½"
                })
            elif coverage < 80:
                suggestions.append({
                    "category": "ç´¢å¼•ä¼˜åŒ–",
                    "priority": "ä¸­",
                    "model": model_name,
                    "issue": f"ç´¢å¼•è¦†ç›–ç‡ä¸€èˆ¬ ({coverage}%)",
                    "suggestion": f"è€ƒè™‘ä¸ºå…³é”®å­—æ®µæ·»åŠ ç´¢å¼•: {', '.join(missing_indexes[:3])}",
                    "impact": "æå‡æŸ¥è¯¢æ€§èƒ½"
                })

        # åŸºäºæŸ¥è¯¢æ¨¡å¼çš„å»ºè®®
        query_patterns = db_analysis.get("query_patterns", {})
        slow_queries = query_patterns.get("potential_slow_queries", [])
        join_operations = query_patterns.get("join_operations", [])

        if len(slow_queries) > 10:
            suggestions.append({
                "category": "æŸ¥è¯¢ä¼˜åŒ–",
                "priority": "é«˜",
                "issue": f"å‘ç° {len(slow_queries)} ä¸ªæ½œåœ¨æ…¢æŸ¥è¯¢",
                "suggestion": "ä¼˜åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œæ·»åŠ å¤åˆç´¢å¼•ï¼Œè€ƒè™‘æŸ¥è¯¢é‡æ„",
                "impact": "å¤§å¹…æå‡æŸ¥è¯¢é€Ÿåº¦"
            })

        if len(join_operations) > 15:
            suggestions.append({
                "category": "è¿æ¥ä¼˜åŒ–",
                "priority": "ä¸­",
                "issue": f"å‘ç° {len(join_operations)} ä¸ªè¿æ¥æ“ä½œ",
                "suggestion": "ä½¿ç”¨é¢„åŠ è½½(eager loading)ä¼˜åŒ–è¿æ¥æŸ¥è¯¢",
                "impact": "å‡å°‘N+1æŸ¥è¯¢é—®é¢˜"
            })

        # é€šç”¨ä¼˜åŒ–å»ºè®®
        suggestions.extend([
            {
                "category": "æ•°æ®åº“é…ç½®",
                "priority": "ä¸­",
                "issue": "éœ€è¦æ•°æ®åº“è¿æ¥æ± é…ç½®",
                "suggestion": "é…ç½®åˆé€‚çš„è¿æ¥æ± å¤§å°å’Œè¶…æ—¶è®¾ç½®",
                "impact": "æå‡å¹¶å‘æ€§èƒ½"
            },
            {
                "category": "æŸ¥è¯¢ç¼“å­˜",
                "priority": "é«˜",
                "issue": "ç¼ºå°‘æŸ¥è¯¢ç¼“å­˜æœºåˆ¶",
                "suggestion": "å®ç°RedisæŸ¥è¯¢ç¼“å­˜ï¼Œç¼“å­˜é¢‘ç¹æŸ¥è¯¢ç»“æœ",
                "impact": "å¤§å¹…æå‡é‡å¤æŸ¥è¯¢æ€§èƒ½"
            },
            {
                "category": "æ•°æ®åº“åˆ†åŒº",
                "priority": "ä½",
                "issue": "å¤§è¡¨ç¼ºå°‘åˆ†åŒºç­–ç•¥",
                "suggestion": "å¯¹æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œåˆ†åŒºï¼Œæå‡æŸ¥è¯¢æ•ˆç‡",
                "impact": "ä¼˜åŒ–å¤§æ•°æ®é‡æŸ¥è¯¢"
            }
        ])

        db_analysis["optimization_suggestions"] = suggestions

    def analyze_caching_strategy(self):
        """åˆ†æç¼“å­˜ç­–ç•¥"""
        print("ğŸ’¾ åˆ†æç¼“å­˜ç­–ç•¥...")

        cache_analysis = {
            "redis_usage": 0,
            "cache_files": 0,
            "cache_strategies": [],
            "optimization_opportunities": []
        }

        try:
            # æŸ¥æ‰¾Redisç›¸å…³æ–‡ä»¶
            redis_files = list(self.backend_root.rglob("*redis*.py"))
            cache_files = list(self.backend_root.rglob("*cache*.py"))

            cache_analysis["redis_files"] = len(redis_files)
            cache_analysis["cache_files"] = len(cache_files)

            # åˆ†æç¼“å­˜ä½¿ç”¨
            all_files = list(self.backend_root.rglob("*.py"))
            redis_usage = 0
            cache_patterns = []

            for file in all_files[:50]:  # é‡‡æ ·åˆ†æ
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if 'redis' in content.lower():
                        redis_usage += content.lower().count('redis')

                    # æŸ¥æ‰¾ç¼“å­˜æ¨¡å¼
                    if 'cache' in content.lower():
                        cache_patterns.append({
                            "file": str(file.relative_to(self.backend_root)),
                            "patterns": self._extract_cache_patterns(content)
                        })

                except:
                    continue

            cache_analysis["redis_usage"] = redis_usage
            cache_analysis["cache_patterns"] = cache_patterns

            # ç”Ÿæˆç¼“å­˜ä¼˜åŒ–å»ºè®®
            if redis_usage < 5:
                cache_analysis["optimization_opportunities"].append({
                    "category": "Redisé›†æˆ",
                    "priority": "é«˜",
                    "issue": "Redisä½¿ç”¨ä¸è¶³",
                    "suggestion": "å¢åŠ Redisç¼“å­˜ä½¿ç”¨ï¼Œç¼“å­˜æŸ¥è¯¢ç»“æœå’Œä¼šè¯æ•°æ®",
                    "impact": "å¤§å¹…æå‡å“åº”é€Ÿåº¦"
                })

            if len(cache_files) < 2:
                cache_analysis["optimization_opportunities"].append({
                    "category": "ç¼“å­˜æ¶æ„",
                    "priority": "ä¸­",
                    "issue": "ç¼“å­˜æ¶æ„ä¸å®Œå–„",
                    "suggestion": "å»ºç«‹ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†æ¶æ„",
                    "impact": "æé«˜ç¼“å­˜åˆ©ç”¨ç‡å’Œä¸€è‡´æ€§"
                })

        except Exception as e:
            print(f"ç¼“å­˜ç­–ç•¥åˆ†æå¤±è´¥: {e}")

        self.report["analysis"]["caching"] = cache_analysis

    def _extract_cache_patterns(self, content: str) -> List[str]:
        """æå–ç¼“å­˜æ¨¡å¼"""
        patterns = []
        cache_keywords = [
            'cache.get',
            'cache.set',
            'cache.delete',
            '@cache',
            'redis.get',
            'redis.set',
            'redis.delete',
            'memoize'
        ]

        for keyword in cache_keywords:
            if keyword in content:
                patterns.append(keyword)

        return patterns

    def calculate_optimization_score(self):
        """è®¡ç®—ä¼˜åŒ–è¯„åˆ†"""
        print("ğŸ“ˆ è®¡ç®—æ•°æ®åº“ä¼˜åŒ–è¯„åˆ†...")

        scores = {
            "index_optimization": 70,
            "query_optimization": 75,
            "caching_strategy": 60,
            "schema_design": 80
        }

        # åŸºäºåˆ†æç»“æœè°ƒæ•´åˆ†æ•°
        db_design = self.report["analysis"].get("database_design", {})
        cache_analysis = self.report["analysis"].get("caching", {})

        # ç´¢å¼•ä¼˜åŒ–è¯„åˆ†
        index_coverage = db_design.get("index_coverage", {})
        if index_coverage:
            avg_coverage = sum(info.get("coverage_percent", 0) for info in index_coverage.values()) / len(index_coverage)
            if avg_coverage > 80:
                scores["index_optimization"] = 90
            elif avg_coverage > 60:
                scores["index_optimization"] = 80
            else:
                scores["index_optimization"] = 60

        # ç¼“å­˜ç­–ç•¥è¯„åˆ†
        redis_usage = cache_analysis.get("redis_usage", 0)
        if redis_usage > 10:
            scores["caching_strategy"] = 85
        elif redis_usage > 5:
            scores["caching_strategy"] = 75
        else:
            scores["caching_strategy"] = 50

        # æŸ¥è¯¢ä¼˜åŒ–è¯„åˆ†
        query_patterns = db_design.get("query_patterns", {})
        slow_queries = len(query_patterns.get("potential_slow_queries", []))
        if slow_queries < 5:
            scores["query_optimization"] = 85
        elif slow_queries < 15:
            scores["query_optimization"] = 75
        else:
            scores["query_optimization"] = 65

        # è®¡ç®—æ€»åˆ†
        total_score = sum(scores.values()) / len(scores)

        self.report["optimization_score"] = {
            "total_score": round(total_score, 1),
            "grade": self._get_grade(total_score),
            "individual_scores": scores
        }

    def _get_grade(self, score: float) -> str:
        """è·å–ç­‰çº§"""
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (ä¸€èˆ¬)"
        elif score >= 60:
            return "C (éœ€è¦æ”¹è¿›)"
        else:
            return "D (æ€¥éœ€ä¼˜åŒ–)"

    def generate_report(self):
        """ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š...")

        # ç”ŸæˆJSONæŠ¥å‘Š
        json_path = self.project_root / "database_optimization_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_path = self.project_root / "database_optimization_report.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(self._create_markdown_report())

        return str(json_path)

    def _create_markdown_report(self) -> str:
        """åˆ›å»ºMarkdownæŠ¥å‘Š"""
        report = self.report
        score = report.get("optimization_score", {})

        md = f"""# å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æ•°æ®åº“ä¼˜åŒ–åˆ†ææŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚è§ˆ

**åˆ†ææ—¶é—´**: {report["timestamp"]}
**é¡¹ç›®è·¯å¾„**: {report["project_root"]}

### ğŸ¯ ä¼˜åŒ–è¯„åˆ†
**æ€»åˆ†**: {score.get("total_score", "N/A")}/100 ({score.get("grade", "N/A")})

| ç»´åº¦ | å¾—åˆ† | è¯„ä»· |
|------|------|------|
| ç´¢å¼•ä¼˜åŒ– | {score.get("individual_scores", {}).get("index_optimization", "N/A")} | æ•°æ®åº“ç´¢å¼•è¦†ç›–ç‡ |
| æŸ¥è¯¢ä¼˜åŒ– | {score.get("individual_scores", {}).get("query_optimization", "N/A")} | æŸ¥è¯¢æ•ˆç‡å’Œæ¨¡å¼ |
| ç¼“å­˜ç­–ç•¥ | {score.get("individual_scores", {}).get("caching_strategy", "N/A")} | ç¼“å­˜æ¶æ„å’Œä½¿ç”¨ |
| æ¨¡å¼è®¾è®¡ | {score.get("individual_scores", {}).get("schema_design", "N/A")} | æ•°æ®åº“ç»“æ„è®¾è®¡ |

---

## ğŸ—„ï¸ æ•°æ®åº“è®¾è®¡åˆ†æ

### ğŸ“Š æ¨¡å‹ç»Ÿè®¡
"""

        db_design = report["analysis"].get("database_design", {})
        models = db_design.get("models", {})

        md += f"- **æ€»æ¨¡å‹æ•°**: {len(models)}\n"

        for model_name, model_info in models.items():
            md += f"- **{model_name}**:\n"
            md += f"  - è¡¨å: {', '.join(model_info.get('table_names', ['N/A']))}\n"
            md += f"  - å­—æ®µæ•°: {len(model_info.get('fields', []))}\n"
            md += f"  - å…³ç³»æ•°: {len(model_info.get('relationships', []))}\n"
            md += f"  - ç´¢å¼•æ•°: {len(model_info.get('indexes', []))}\n"

        md += f"""
### ğŸ“Š ç´¢å¼•è¦†ç›–ç‡åˆ†æ
"""

        index_coverage = db_design.get("index_coverage", {})
        for model_name, coverage_info in index_coverage.items():
            coverage = coverage_info.get("coverage_percent", 0)
            md += f"- **{model_name}**: {coverage}% è¦†ç›–ç‡\n"
            md += f"  - æ€»å­—æ®µ: {coverage_info.get('total_fields', 0)}\n"
            md += f"  - å·²ç´¢å¼•: {coverage_info.get('indexed_fields', 0)}\n"
            if coverage_info.get('missing_indexes'):
                md += f"  - ç¼ºå¤±ç´¢å¼•: {', '.join(coverage_info['missing_indexes'][:5])}\n"

        md += f"""
### ğŸ” æŸ¥è¯¢æ¨¡å¼åˆ†æ
"""

        query_patterns = db_design.get("query_patterns", {})
        md += f"- **æ½œåœ¨æ…¢æŸ¥è¯¢**: {len(query_patterns.get('potential_slow_queries', []))} ä¸ª\n"
        md += f"- **è¿æ¥æ“ä½œ**: {len(query_patterns.get('join_operations', []))} ä¸ª\n"
        md += f"- **è¿‡æ»¤æ“ä½œ**: {len(query_patterns.get('filter_operations', []))} ä¸ª\n"

        md += f"""

---

## ğŸ’¾ ç¼“å­˜ç­–ç•¥åˆ†æ

### ğŸ“Š ç¼“å­˜ç»Ÿè®¡
"""

        cache_analysis = report["analysis"].get("caching", {})
        md += f"- **Redisæ–‡ä»¶æ•°**: {cache_analysis.get('redis_files', 0)}\n"
        md += f"- **ç¼“å­˜æ–‡ä»¶æ•°**: {cache_analysis.get('cache_files', 0)}\n"
        md += f"- **Redisä½¿ç”¨æ¬¡æ•°**: {cache_analysis.get('redis_usage', 0)}\n"

        md += f"""
### ğŸ“‹ ç¼“å­˜æ¨¡å¼
"""

        cache_patterns = cache_analysis.get("cache_patterns", [])
        for pattern_info in cache_patterns[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            md += f"- `{pattern_info['file']}`: {', '.join(pattern_info['patterns'])}\n"

        md += f"""

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### ğŸ¯ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        # åˆå¹¶æ•°æ®åº“å’Œç¼“å­˜çš„ä¼˜åŒ–å»ºè®®
        all_recommendations = []
        db_suggestions = db_design.get("optimization_suggestions", [])
        cache_suggestions = cache_analysis.get("optimization_opportunities", [])
        all_recommendations.extend(db_suggestions)
        all_recommendations.extend(cache_suggestions)

        high_priority = [r for r in all_recommendations if r.get("priority") == "é«˜"]
        for rec in high_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **é¢„æœŸå½±å“**: {rec.get("impact", "æ— ")}
"""

        md += """
### ğŸ“‹ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        medium_priority = [r for r in all_recommendations if r.get("priority") == "ä¸­"]
        for rec in medium_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **é¢„æœŸå½±å“**: {rec.get("impact", "æ— ")}
"""

        md += """
### ğŸ“ ä½ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        low_priority = [r for r in all_recommendations if r.get("priority") == "ä½"]
        for rec in low_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **é¢„æœŸå½±å“**: {rec.get("impact", "æ— ")}
"""

        md += f"""

---

## ğŸš€ å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µ (ç«‹å³æ‰§è¡Œ)
1. **æ·»åŠ ç¼ºå¤±ç´¢å¼•**: ä¸ºå…³é”®æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•
2. **å®æ–½æŸ¥è¯¢ç¼“å­˜**: ç¼“å­˜é¢‘ç¹æŸ¥è¯¢çš„ç»“æœ
3. **ä¼˜åŒ–æ…¢æŸ¥è¯¢**: é‡æ„å¤æ‚çš„æŸ¥è¯¢é€»è¾‘

### ç¬¬äºŒé˜¶æ®µ (1-2å‘¨å†…)
1. **å®Œå–„ç¼“å­˜æ¶æ„**: å»ºç«‹ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†
2. **æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–**: é…ç½®åˆé€‚çš„è¿æ¥å‚æ•°
3. **æŸ¥è¯¢æ€§èƒ½ç›‘æ§**: å»ºç«‹æŸ¥è¯¢æ€§èƒ½ç›‘æ§æœºåˆ¶

### ç¬¬ä¸‰é˜¶æ®µ (é•¿æœŸè§„åˆ’)
1. **æ•°æ®åº“åˆ†åŒº**: å¯¹å¤§æ•°æ®é‡è¡¨è¿›è¡Œåˆ†åŒº
2. **è¯»å†™åˆ†ç¦»**: å®ç°ä¸»ä»æ•°æ®åº“æ¶æ„
3. **æ•°æ®å½’æ¡£**: å»ºç«‹å†å²æ•°æ®å½’æ¡£ç­–ç•¥

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### ğŸ¯ æ€§èƒ½æå‡
- **æŸ¥è¯¢é€Ÿåº¦**: æå‡ 50-80%
- **å¹¶å‘èƒ½åŠ›**: æå‡ 30-50%
- **å“åº”æ—¶é—´**: å‡å°‘ 40-60%
- **ç³»ç»Ÿç¨³å®šæ€§**: æ˜¾è‘—æå‡

### ğŸ’° æˆæœ¬èŠ‚çº¦
- **æ•°æ®åº“è´Ÿè½½**: å‡å°‘ 40-60%
- **æœåŠ¡å™¨èµ„æº**: èŠ‚çº¦ 20-30%
- **è¿ç»´æˆæœ¬**: é™ä½ 25-35%

---

**ğŸ¯ æ•°æ®åº“ä¼˜åŒ–è¯„åˆ†: {score.get("total_score", "N/A")}/100 ({score.get("grade", "N/A")})**

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report["timestamp"]}*
"""

        return md

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="è¿è¡Œæ•°æ®åº“ä¼˜åŒ–åˆ†æ")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")

    args = parser.parse_args()

    print("ğŸš€ å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æ•°æ®åº“ä¼˜åŒ–åˆ†æå™¨")
    print("=" * 60)

    analyzer = DatabaseOptimizationAnalyzer(args.project_root)

    try:
        # è¿è¡Œåˆ†æ
        analyzer.analyze_database_design()
        analyzer.analyze_caching_strategy()
        analyzer.calculate_optimization_score()

        # ç”ŸæˆæŠ¥å‘Š
        report_path = analyzer.generate_report()

        # æ˜¾ç¤ºç»“æœ
        score = analyzer.report.get("optimization_score", {}).get("total_score", "N/A")
        grade = analyzer.report.get("optimization_score", {}).get("grade", "N/A")

        print(f"\nğŸ“Š æ•°æ®åº“ä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ˆ ä¼˜åŒ–è¯„åˆ†: {score}/100 ({grade})")
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°:")
        print(f"   JSON: {report_path}")
        print(f"   Markdown: {report_path.replace('.json', '.md')}")

        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        db_design = analyzer.report["analysis"].get("database_design", {})
        cache_analysis = analyzer.report["analysis"].get("caching", {})

        print(f"\nğŸ“Š å…³é”®ç»Ÿè®¡:")
        print(f"   æ•°æ®æ¨¡å‹: {len(db_design.get('models', {}))} ä¸ª")
        print(f"   Redisä½¿ç”¨: {cache_analysis.get('redis_usage', 0)} æ¬¡")
        print(f"   ç¼“å­˜æ–‡ä»¶: {cache_analysis.get('cache_files', 0)} ä¸ª")
        print(f"   æ½œåœ¨æ…¢æŸ¥è¯¢: {len(db_design.get('query_patterns', {}).get('potential_slow_queries', []))} ä¸ª")

        print("=" * 60)

    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()