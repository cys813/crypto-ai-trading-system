#!/usr/bin/env python3
"""
æ€§èƒ½åˆ†æè„šæœ¬
åˆ†æç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼Œè¯†åˆ«ä¼˜åŒ–ç‚¹
"""

import os
import sys
import time
import json
import asyncio
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    code_complexity: Dict[str, Any]
    function_analysis: Dict[str, Any]
    dependency_analysis: Dict[str, Any]
    database_optimization: Dict[str, Any]
    api_performance: Dict[str, Any]
    memory_usage: Dict[str, Any]
    execution_time: Dict[str, Any]

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.backend_root = self.project_root / "backend"
        self.report = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(project_root),
            "analysis": {},
            "recommendations": [],
            "metrics": {}
        }

    async def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½åˆ†æ"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½åˆ†æ...")

        # ä»£ç å¤æ‚åº¦åˆ†æ
        print("ğŸ“Š åˆ†æä»£ç å¤æ‚åº¦...")
        complexity = await self.analyze_code_complexity()
        self.report["analysis"]["code_complexity"] = complexity

        # å‡½æ•°æ€§èƒ½åˆ†æ
        print("âš¡ åˆ†æå‡½æ•°æ€§èƒ½...")
        function_analysis = await self.analyze_function_performance()
        self.report["analysis"]["function_performance"] = function_analysis

        # ä¾èµ–å…³ç³»åˆ†æ
        print("ğŸ”— åˆ†æä¾èµ–å…³ç³»...")
        dependency_analysis = await self.analyze_dependencies()
        self.report["analysis"]["dependencies"] = dependency_analysis

        # æ•°æ®åº“ä¼˜åŒ–åˆ†æ
        print("ğŸ—„ï¸ åˆ†ææ•°æ®åº“ä¼˜åŒ–...")
        db_analysis = await self.analyze_database_optimization()
        self.report["analysis"]["database"] = db_analysis

        # APIæ€§èƒ½åˆ†æ
        print("ğŸŒ åˆ†æAPIæ€§èƒ½...")
        api_analysis = await self.analyze_api_performance()
        self.report["analysis"]["api_performance"] = api_analysis

        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        recommendations = await self.generate_recommendations()
        self.report["recommendations"] = recommendations

        # è®¡ç®—æ€§èƒ½è¯„åˆ†
        print("ğŸ“ˆ è®¡ç®—æ€§èƒ½è¯„åˆ†...")
        performance_score = await self.calculate_performance_score()
        self.report["performance_score"] = performance_score

        return self.report

    async def analyze_code_complexity(self) -> Dict[str, Any]:
        """åˆ†æä»£ç å¤æ‚åº¦"""
        complexity_metrics = {
            "total_files": 0,
            "total_lines": 0,
            "largest_files": [],
            "complex_functions": [],
            "average_function_length": 0,
            "cyclomatic_complexity": {}
        }

        try:
            # è·å–æ‰€æœ‰Pythonæ–‡ä»¶
            python_files = list(self.backend_root.rglob("*.py"))
            complexity_metrics["total_files"] = len(python_files)

            # åˆ†ææ–‡ä»¶å¤§å°
            file_sizes = []
            for py_file in python_files:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        lines = len(f.readlines())
                        file_sizes.append((str(py_file.relative_to(self.backend_root)), lines))
                except:
                    pass

            file_sizes.sort(key=lambda x: x[1], reverse=True)
            complexity_metrics["largest_files"] = file_sizes[:10]
            complexity_metrics["total_lines"] = sum(size for _, size in file_sizes)

            # åˆ†æå‡½æ•°å¤æ‚åº¦
            complexity_metrics["average_function_length"] = self._calculate_average_function_length(python_files[:20])  # é‡‡æ ·åˆ†æ
            complexity_metrics["complex_functions"] = self._identify_complex_functions(python_files[:20])

            # è®¡ç®—åœˆå¤æ‚åº¦æŒ‡æ ‡
            complexity_metrics["cyclomatic_complexity"] = {
                "high_complexity_files": len([f for f in file_sizes if f[1] > 500]),
                "medium_complexity_files": len([f for f in file_sizes if 200 < f[1] <= 500]),
                "low_complexity_files": len([f for f in file_sizes if f[1] <= 200])
            }

        except Exception as e:
            print(f"ä»£ç å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")

        return complexity_metrics

    async def analyze_function_performance(self) -> Dict[str, Any]:
        """åˆ†æå‡½æ•°æ€§èƒ½"""
        function_metrics = {
            "async_functions": 0,
            "sync_functions": 0,
            "database_operations": 0,
            "api_endpoints": 0,
            "background_tasks": 0,
            "critical_functions": []
        }

        try:
            python_files = list(self.backend_root.rglob("*.py"))

            for py_file in python_files[:50]:  # é‡‡æ ·åˆ†æ
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                        # ç»Ÿè®¡å¼‚æ­¥å‡½æ•°
                        function_metrics["async_functions"] += content.count('async def')
                        function_metrics["sync_functions"] += content.count('def ') - content.count('async def')

                        # ç»Ÿè®¡æ•°æ®åº“æ“ä½œ
                        db_keywords = ['session.query(', 'db.query(', 'session.add(', 'session.commit(']
                        function_metrics["database_operations"] += sum(content.count(keyword) for keyword in db_keywords)

                        # ç»Ÿè®¡APIç«¯ç‚¹
                        function_metrics["api_endpoints"] += content.count('@app.') + content.count('@router.')

                        # ç»Ÿè®¡åå°ä»»åŠ¡
                        function_metrics["background_tasks"] += content.count('@celery.task') + content.count('@task')

                        # è¯†åˆ«å…³é”®å‡½æ•°
                        if any(keyword in str(py_file) for keyword in ['trading', 'executor', 'manager', 'strategy']):
                            function_metrics["critical_functions"].append(str(py_file.relative_to(self.backend_root)))

                except:
                    continue

        except Exception as e:
            print(f"å‡½æ•°æ€§èƒ½åˆ†æå¤±è´¥: {e}")

        return function_metrics

    async def analyze_dependencies(self) -> Dict[str, Any]:
        """åˆ†æä¾èµ–å…³ç³»"""
        dependency_metrics = {
            "total_dependencies": 0,
            "external_dependencies": 0,
            "internal_dependencies": 0,
            "heavy_dependencies": [],
            "dependency_graph": {},
            "circular_imports": []
        }

        try:
            requirements_path = self.backend_root / "requirements.txt"
            if requirements_path.exists():
                with open(requirements_path, 'r') as f:
                    lines = f.readlines()

                dependencies = []
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        dependency_metrics["total_dependencies"] += 1
                        dependencies.append(line)

                        # æ£€æŸ¥é‡å‹ä¾èµ–
                        heavy_libs = ['pandas', 'numpy', 'tensorflow', 'torch', 'scikit-learn', 'django']
                        if any(heavy_lib in line.lower() for heavy_lib in heavy_libs):
                            dependency_metrics["heavy_dependencies"].append(line)

                dependency_metrics["external_dependencies"] = len(dependencies)

            # åˆ†æå†…éƒ¨ä¾èµ–
            python_files = list(self.backend_root.rglob("*.py"))
            internal_imports = set()

            for py_file in python_files[:30]:  # é‡‡æ ·åˆ†æ
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                        # æŸ¥æ‰¾å†…éƒ¨å¯¼å…¥
                        for line in content.split('\n'):
                            if 'from ..' in line or 'from .' in line:
                                internal_imports.add(line.strip())

                except:
                    continue

            dependency_metrics["internal_dependencies"] = len(internal_imports)

        except Exception as e:
            print(f"ä¾èµ–å…³ç³»åˆ†æå¤±è´¥: {e}")

        return dependency_metrics

    async def analyze_database_optimization(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åº“ä¼˜åŒ–"""
        db_metrics = {
            "total_models": 0,
            "indexed_fields": 0,
            "relationships": 0,
            "query_optimization": 0,
            "caching_strategies": 0,
            "optimization_suggestions": []
        }

        try:
            models_dir = self.backend_root / "src" / "models"
            if models_dir.exists():
                model_files = list(models_dir.glob("*.py"))
                db_metrics["total_models"] = len(model_files)

                for model_file in model_files:
                    try:
                        with open(model_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                            # ç»Ÿè®¡ç´¢å¼•
                            db_metrics["indexed_fields"] += content.count('Index(')

                            # ç»Ÿè®¡å…³ç³»
                            db_metrics["relationships"] += content.count('relationship(')

                            # æŸ¥è¯¢ä¼˜åŒ–
                            if 'lazy=' in content or 'joinedload' in content:
                                db_metrics["query_optimization"] += 1

                    except:
                        continue

            # æ£€æŸ¥ç¼“å­˜ç­–ç•¥
            cache_files = list(self.backend_root.rglob("cache*.py"))
            redis_files = list(self.backend_root.rglob("*redis*.py"))
            db_metrics["caching_strategies"] = len(cache_files) + len(redis_files)

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            if isinstance(db_metrics["indexed_fields"], int) and isinstance(db_metrics["total_models"], int) and db_metrics["indexed_fields"] < db_metrics["total_models"] * 2:
                db_metrics["optimization_suggestions"].append("å»ºè®®æ·»åŠ æ›´å¤šæ•°æ®åº“ç´¢å¼•")

            if db_metrics["caching_strategies"] == 0:
                db_metrics["optimization_suggestions"].append("å»ºè®®å®ç°Redisç¼“å­˜ç­–ç•¥")

        except Exception as e:
            print(f"æ•°æ®åº“ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")

        return db_metrics

    async def analyze_api_performance(self) -> Dict[str, Any]:
        """åˆ†æAPIæ€§èƒ½"""
        api_metrics = {
            "total_endpoints": 0,
            "async_endpoints": 0,
            "middleware_count": 0,
            "validation_complexity": 0,
            "response_compression": 0,
            "rate_limiting": 0,
            "performance_features": []
        }

        try:
            api_dir = self.backend_root / "src" / "api"
            if api_dir.exists():
                api_files = list(api_dir.rglob("*.py"))

                for api_file in api_files:
                    try:
                        with open(api_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                            # ç»Ÿè®¡APIç«¯ç‚¹
                            api_metrics["total_endpoints"] += content.count('@app.') + content.count('@router.')
                            api_metrics["async_endpoints"] += content.count('async def')

                            # ä¸­é—´ä»¶
                            api_metrics["middleware_count"] += content.count('@middleware') + content.count('@app.middleware')

                            # éªŒè¯å¤æ‚åº¦
                            api_metrics["validation_complexity"] += content.count('Pydantic') + content.count('validator')

                            # æ€§èƒ½ç‰¹æ€§
                            if 'gzip' in content.lower():
                                api_metrics["response_compression"] += 1
                                api_metrics["performance_features"].append("å“åº”å‹ç¼©")

                            if 'rate' in content.lower() and 'limit' in content.lower():
                                api_metrics["rate_limiting"] += 1
                                api_metrics["performance_features"].append("é™æµä¿æŠ¤")

                    except:
                        continue

        except Exception as e:
            print(f"APIæ€§èƒ½åˆ†æå¤±è´¥: {e}")

        return api_metrics

    async def generate_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºä»£ç å¤æ‚åº¦çš„å»ºè®®
        complexity = self.report["analysis"].get("code_complexity", {})
        if complexity.get("total_lines", 0) > 25000:
            recommendations.append({
                "category": "ä»£ç å¤æ‚åº¦",
                "priority": "é«˜",
                "issue": "ä»£ç åº“è¿‡å¤§",
                "suggestion": "è€ƒè™‘æ¨¡å—åŒ–æ‹†åˆ†ï¼Œå°†å¤§å‹æ¨¡å—åˆ†è§£ä¸ºç‹¬ç«‹çš„æœåŠ¡",
                "impact": "æé«˜å¯ç»´æŠ¤æ€§å’Œå¼€å‘æ•ˆç‡"
            })

        large_files = complexity.get("largest_files", [])
        if large_files and len(large_files) > 0 and isinstance(large_files[0], tuple) and len(large_files[0]) == 2 and large_files[0][1] > 1000:
            recommendations.append({
                "category": "æ–‡ä»¶å¤§å°",
                "priority": "ä¸­",
                "issue": f"å­˜åœ¨è¶…å¤§æ–‡ä»¶: {large_files[0][0]} ({large_files[0][1]}è¡Œ)",
                "suggestion": "å°†å¤§æ–‡ä»¶æ‹†åˆ†ä¸ºå¤šä¸ªå°æ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ä¸“æ³¨äºå•ä¸€èŒè´£",
                "impact": "æé«˜ä»£ç å¯è¯»æ€§å’Œç»´æŠ¤æ€§"
            })

        # åŸºäºå‡½æ•°æ€§èƒ½çš„å»ºè®®
        function_perf = self.report["analysis"].get("function_performance", {})
        async_funcs = function_perf.get("async_functions", 0) if isinstance(function_perf.get("async_functions"), int) else 0
        sync_funcs = function_perf.get("sync_functions", 0) if isinstance(function_perf.get("sync_functions"), int) else 1
        async_ratio = async_funcs / max(sync_funcs, 1)
        if async_ratio < 0.3:
            recommendations.append({
                "category": "å¼‚æ­¥ç¼–ç¨‹",
                "priority": "ä¸­",
                "issue": "å¼‚æ­¥å‡½æ•°æ¯”ä¾‹è¾ƒä½",
                "suggestion": "åœ¨I/Oå¯†é›†å‹æ“ä½œä¸­ä½¿ç”¨async/awaitæé«˜å¹¶å‘æ€§èƒ½",
                "impact": "æ˜¾è‘—æé«˜ç³»ç»Ÿååé‡"
            })

        # åŸºäºæ•°æ®åº“çš„å»ºè®®
        db_analysis = self.report["analysis"].get("database", {})
        if db_analysis.get("caching_strategies", 0) == 0:
            recommendations.append({
                "category": "ç¼“å­˜ç­–ç•¥",
                "priority": "é«˜",
                "issue": "ç¼ºå°‘ç¼“å­˜æœºåˆ¶",
                "suggestion": "å®ç°Redisç¼“å­˜ä»¥å‡å°‘æ•°æ®åº“æŸ¥è¯¢å’Œæé«˜å“åº”é€Ÿåº¦",
                "impact": "å¤§å¹…æå‡æŸ¥è¯¢æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ"
            })

        # åŸºäºä¾èµ–çš„å»ºè®®
        deps = self.report["analysis"].get("dependencies", {})
        if deps.get("heavy_dependencies", 0) > 5:
            recommendations.append({
                "category": "ä¾èµ–ä¼˜åŒ–",
                "priority": "ä½",
                "issue": "é‡å‹ä¾èµ–è¾ƒå¤š",
                "suggestion": "è€ƒè™‘ä½¿ç”¨è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆæˆ–æŒ‰éœ€åŠ è½½é‡å‹åº“",
                "impact": "å‡å°‘å†…å­˜å ç”¨å’Œå¯åŠ¨æ—¶é—´"
            })

        # é€šç”¨ä¼˜åŒ–å»ºè®®
        recommendations.extend([
            {
                "category": "ç›‘æ§",
                "priority": "é«˜",
                "issue": "ç¼ºå°‘æ€§èƒ½ç›‘æ§",
                "suggestion": "é›†æˆAPMå·¥å…·(å¦‚Sentry, New Relic)ç›‘æ§ç³»ç»Ÿæ€§èƒ½",
                "impact": "åŠæ—¶å‘ç°å’Œè§£å†³æ€§èƒ½é—®é¢˜"
            },
            {
                "category": "æµ‹è¯•",
                "priority": "ä¸­",
                "issue": "éœ€è¦æ€§èƒ½æµ‹è¯•",
                "suggestion": "æ·»åŠ è´Ÿè½½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§",
                "impact": "éªŒè¯ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„è¡¨ç°"
            },
            {
                "category": "éƒ¨ç½²",
                "priority": "ä¸­",
                "issue": "ä¼˜åŒ–éƒ¨ç½²ç­–ç•¥",
                "suggestion": "ä½¿ç”¨å®¹å™¨åŒ–å’Œå¾®æœåŠ¡æ¶æ„æé«˜éƒ¨ç½²æ•ˆç‡",
                "impact": "æé«˜ç³»ç»Ÿå¯æ‰©å±•æ€§å’Œç»´æŠ¤æ€§"
            }
        ])

        return recommendations

    async def calculate_performance_score(self) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        scores = {}

        # ä»£ç è´¨é‡è¯„åˆ† (40%)
        complexity = self.report["analysis"].get("code_complexity", {})
        code_score = 100

        if complexity.get("total_lines", 0) > 30000:
            code_score -= 20

        large_files = complexity.get("largest_files", [])
        if large_files and len(large_files) > 0 and isinstance(large_files[0], tuple) and len(large_files[0]) == 2 and large_files[0][1] > 1000:
            code_score -= 15

        avg_len = complexity.get("average_function_length", 0)
        if isinstance(avg_len, (int, float)) and avg_len > 50:
            code_score -= 10

        scores["code_quality"] = max(0, code_score)

        # æ€§èƒ½è®¾è®¡è¯„åˆ† (30%)
        function_perf = self.report["analysis"].get("function_performance", {})
        perf_score = 100

        async_funcs = function_perf.get("async_functions", 0) if isinstance(function_perf.get("async_functions"), int) else 0
        sync_funcs = function_perf.get("sync_functions", 0) if isinstance(function_perf.get("sync_functions"), int) else 0
        if async_funcs < sync_funcs:
            perf_score -= 20

        db_ops = function_perf.get("database_operations", 0) if isinstance(function_perf.get("database_operations"), int) else 0
        if db_ops > 100:
            perf_score -= 15

        scores["performance_design"] = max(0, perf_score)

        # æ¶æ„ä¼˜åŒ–è¯„åˆ† (30%)
        db_analysis = self.report["analysis"].get("database", {})
        api_analysis = self.report["analysis"].get("api_performance", {})
        arch_score = 100

        if db_analysis.get("caching_strategies", 0) == 0:
            arch_score -= 25

        if api_analysis.get("rate_limiting", 0) == 0:
            arch_score -= 15

        if api_analysis.get("response_compression", 0) == 0:
            arch_score -= 10

        scores["architecture_optimization"] = max(0, arch_score)

        # ç»¼åˆè¯„åˆ†
        total_score = (
            scores["code_quality"] * 0.4 +
            scores["performance_design"] * 0.3 +
            scores["architecture_optimization"] * 0.3
        )

        return {
            "total_score": round(total_score, 1),
            "grade": self._get_performance_grade(total_score),
            "individual_scores": scores,
            "analysis": self._get_score_analysis(total_score)
        }

    def _calculate_average_function_length(self, files: List[Path]) -> float:
        """è®¡ç®—å¹³å‡å‡½æ•°é•¿åº¦"""
        total_functions = 0
        total_lines = 0

        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    in_function = False
                    function_lines = 0

                    for line in lines:
                        stripped = line.strip()
                        if stripped.startswith('def ') or stripped.startswith('async def'):
                            in_function = True
                            function_lines = 1
                        elif in_function:
                            function_lines += 1
                            if stripped and not stripped.startswith(' ') and not stripped.startswith('\t'):
                                # å‡½æ•°ç»“æŸ
                                if function_lines > 1:
                                    total_functions += 1
                                    total_lines += function_lines
                                in_function = False
                    # å¤„ç†æ–‡ä»¶æœ«å°¾çš„å‡½æ•°
                    if in_function and function_lines > 1:
                        total_functions += 1
                        total_lines += function_lines

            except:
                continue

        return total_lines / max(total_functions, 1)

    def _identify_complex_functions(self, files: List[Path]) -> List[Tuple[str, int]]:
        """è¯†åˆ«å¤æ‚å‡½æ•°"""
        complex_functions = []

        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    in_function = False
                    function_lines = 0
                    function_name = ""

                    for i, line in enumerate(lines):
                        stripped = line.strip()
                        if stripped.startswith('def ') or stripped.startswith('async def'):
                            in_function = True
                            function_lines = 1
                            # æå–å‡½æ•°å
                            func_line = stripped.split('(')[0]
                            function_name = func_line.split('def ')[-1]
                        elif in_function:
                            function_lines += 1
                            if stripped and not stripped.startswith(' ') and not stripped.startswith('\t'):
                                # å‡½æ•°ç»“æŸ
                                if function_lines > 50:  # è¶…è¿‡50è¡Œçš„å‡½æ•°è®¤ä¸ºæ˜¯å¤æ‚çš„
                                    complex_functions.append((f"{file.name}:{function_name}", function_lines))
                                in_function = False
                    # å¤„ç†æ–‡ä»¶æœ«å°¾çš„å‡½æ•°
                    if in_function and function_lines > 50:
                        complex_functions.append((f"{file.name}:{function_name}", function_lines))

            except:
                continue

        # è¿”å›æœ€å¤æ‚çš„10ä¸ªå‡½æ•°
        complex_functions.sort(key=lambda x: x[1], reverse=True)
        return complex_functions[:10]

    def _get_performance_grade(self, score: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (ä¸€èˆ¬)"
        elif score >= 60:
            return "C (éœ€è¦æ”¹è¿›)"
        else:
            return "D (æ€¥éœ€ä¼˜åŒ–)"

    def _get_score_analysis(self, score: float) -> str:
        """è·å–è¯„åˆ†åˆ†æ"""
        if score >= 90:
            return "ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œæ¶æ„è®¾è®¡åˆç†ï¼Œä»£ç è´¨é‡é«˜"
        elif score >= 80:
            return "ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå­˜åœ¨å°‘é‡ä¼˜åŒ–ç©ºé—´"
        elif score >= 70:
            return "ç³»ç»Ÿæ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–"
        elif score >= 60:
            return "ç³»ç»Ÿå­˜åœ¨æ˜æ˜¾æ€§èƒ½é—®é¢˜ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–"
        else:
            return "ç³»ç»Ÿå­˜åœ¨ä¸¥é‡æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ç«‹å³è¿›è¡Œä¼˜åŒ–"

    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        report_path = self.project_root / "performance_analysis_report.json"

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        markdown_path = self.project_root / "performance_analysis_report.md"
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(self._create_markdown_report())

        return str(report_path)

    def _create_markdown_report(self) -> str:
        """åˆ›å»ºMarkdownæŠ¥å‘Š"""
        report = self.report
        score_data = report.get("performance_score", {})

        md = f"""# å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æ€§èƒ½åˆ†ææŠ¥å‘Š

## ğŸ“Š æ€§èƒ½åˆ†ææ±‡æ€»

**åˆ†ææ—¶é—´**: {report["timestamp"]}
**é¡¹ç›®è·¯å¾„**: {report["project_root"]}
**æ€§èƒ½è¯„åˆ†**: {score_data.get("total_score", "N/A")}/100 ({score_data.get("grade", "N/A")})

---

## ğŸ¯ ç»¼åˆè¯„ä¼°

{score_data.get("analysis", "æš‚æ— åˆ†æ")}

### ğŸ“ˆ è¯„åˆ†è¯¦æƒ…

| è¯„ä¼°ç»´åº¦ | å¾—åˆ† | æƒé‡ | è¯´æ˜ |
|----------|------|------|------|
| ä»£ç è´¨é‡ | {score_data.get("individual_scores", {}).get("code_quality", "N/A")} | 40% | ä»£ç å¤æ‚åº¦ã€å¯ç»´æŠ¤æ€§ |
| æ€§èƒ½è®¾è®¡ | {score_data.get("individual_scores", {}).get("performance_design", "N/A")} | 30% | å¼‚æ­¥ç¼–ç¨‹ã€æ•°æ®åº“ä¼˜åŒ– |
| æ¶æ„ä¼˜åŒ– | {score_data.get("individual_scores", {}).get("architecture_optimization", "N/A")} | 30% | ç¼“å­˜ç­–ç•¥ã€APIæ€§èƒ½ |

---

## ğŸ“Š ä»£ç å¤æ‚åº¦åˆ†æ

### ğŸ—ï¸ é¡¹ç›®è§„æ¨¡
- **æ€»æ–‡ä»¶æ•°**: {report["analysis"].get("code_complexity", {}).get("total_files", "N/A")}
- **æ€»ä»£ç è¡Œæ•°**: {report["analysis"].get("code_complexity", {}).get("total_lines", "N/A")}

### ğŸ“‹ æœ€å¤§æ–‡ä»¶ (Top 10)
"""

        largest_files = report["analysis"].get("code_complexity", {}).get("largest_files", [])
        for i, (file_path, lines) in enumerate(largest_files[:10], 1):
            md += f"{i}. `{file_path}` - {lines} è¡Œ\n"

        md += f"""
### ğŸ”§ å‡½æ•°å¤æ‚åº¦
- **å¹³å‡å‡½æ•°é•¿åº¦**: {report["analysis"].get("code_complexity", {}).get("average_function_length", "N/A")} è¡Œ
- **åœˆå¤æ‚åº¦åˆ†å¸ƒ**:
  - é«˜å¤æ‚åº¦æ–‡ä»¶(>500è¡Œ): {report["analysis"].get("code_complexity", {}).get("cyclomatic_complexity", {}).get("high_complexity_files", "N/A")}
  - ä¸­ç­‰å¤æ‚åº¦æ–‡ä»¶(200-500è¡Œ): {report["analysis"].get("code_complexity", {}).get("cyclomatic_complexity", {}).get("medium_complexity_files", "N/A")}
  - ä½å¤æ‚åº¦æ–‡ä»¶(â‰¤200è¡Œ): {report["analysis"].get("code_complexity", {}).get("cyclomatic_complexity", {}).get("low_complexity_files", "N/A")}

---

## âš¡ å‡½æ•°æ€§èƒ½åˆ†æ

### ğŸ“ˆ å‡½æ•°ç»Ÿè®¡
- **å¼‚æ­¥å‡½æ•°**: {report["analysis"].get("function_performance", {}).get("async_functions", "N/A")}
- **åŒæ­¥å‡½æ•°**: {report["analysis"].get("function_performance", {}).get("sync_functions", "N/A")}
- **æ•°æ®åº“æ“ä½œ**: {report["analysis"].get("function_performance", {}).get("database_operations", "N/A")}
- **APIç«¯ç‚¹**: {report["analysis"].get("function_performance", {}).get("api_endpoints", "N/A")}
- **åå°ä»»åŠ¡**: {report["analysis"].get("function_performance", {}).get("background_tasks", "N/A")}

---

## ğŸ”— ä¾èµ–å…³ç³»åˆ†æ

### ğŸ“¦ ä¾èµ–ç»Ÿè®¡
- **æ€»ä¾èµ–æ•°**: {report["analysis"].get("dependencies", {}).get("total_dependencies", "N/A")}
- **å¤–éƒ¨ä¾èµ–**: {report["analysis"].get("dependencies", {}).get("external_dependencies", "N/A")}
- **å†…éƒ¨ä¾èµ–**: {report["analysis"].get("dependencies", {}).get("internal_dependencies", "N/A")}

### âš ï¸ é‡å‹ä¾èµ–
"""

        heavy_deps = report["analysis"].get("dependencies", {}).get("heavy_dependencies", [])
        for dep in heavy_deps:
            md += f"- `{dep}`\n"

        md += f"""

---

## ğŸ—„ï¸ æ•°æ®åº“ä¼˜åŒ–åˆ†æ

### ğŸ“Š æ•°æ®åº“ç»Ÿè®¡
- **æ•°æ®æ¨¡å‹æ•°**: {report["analysis"].get("database", {}).get("total_models", "N/A")}
- **ç´¢å¼•å­—æ®µæ•°**: {report["analysis"].get("database", {}).get("indexed_fields", "N/A")}
- **å…³ç³»æ•°é‡**: {report["analysis"].get("database", {}).get("relationships", "N/A")}
- **ç¼“å­˜ç­–ç•¥**: {report["analysis"].get("database", {}).get("caching_strategies", "N/A")}

### ğŸ’¡ æ•°æ®åº“ä¼˜åŒ–å»ºè®®
"""

        db_suggestions = report["analysis"].get("database", {}).get("optimization_suggestions", [])
        for suggestion in db_suggestions:
            md += f"- {suggestion}\n"

        md += f"""

---

## ğŸŒ APIæ€§èƒ½åˆ†æ

### ğŸ“¡ APIç»Ÿè®¡
- **æ€»ç«¯ç‚¹æ•°**: {report["analysis"].get("api_performance", {}).get("total_endpoints", "N/A")}
- **å¼‚æ­¥ç«¯ç‚¹**: {report["analysis"].get("api_performance", {}).get("async_endpoints", "N/A")}
- **ä¸­é—´ä»¶æ•°é‡**: {report["analysis"].get("api_performance", {}).get("middleware_count", "N/A")}
- **å“åº”å‹ç¼©**: {report["analysis"].get("api_performance", {}).get("response_compression", "N/A")}
- **é™æµä¿æŠ¤**: {report["analysis"].get("api_performance", {}).get("rate_limiting", "N/A")}

### ğŸš€ æ€§èƒ½ç‰¹æ€§
"""

        perf_features = report["analysis"].get("api_performance", {}).get("performance_features", [])
        for feature in perf_features:
            md += f"- {feature}\n"

        md += f"""

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### ğŸ¯ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        recommendations = report.get("recommendations", [])
        high_priority = [r for r in recommendations if r.get("priority") == "é«˜"]
        for rec in high_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **å½±å“**: {rec.get("impact", "æ— ")}
"""

        md += """
### ğŸ“‹ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        medium_priority = [r for r in recommendations if r.get("priority") == "ä¸­"]
        for rec in medium_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **å½±å“**: {rec.get("impact", "æ— ")}
"""

        md += """
### ğŸ“ ä½ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        low_priority = [r for r in recommendations if r.get("priority") == "ä½"]
        for rec in low_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **å½±å“**: {rec.get("impact", "æ— ")}
"""

        md += f"""

---

## ğŸ“ˆ æ€»ç»“ä¸å»ºè®®

### ğŸ‰ ç³»ç»Ÿä¼˜åŠ¿
1. **å®Œæ•´çš„æ¶æ„è®¾è®¡**: å¤šAgentç³»ç»Ÿæ¶æ„åˆç†ï¼ŒèŒè´£åˆ†å·¥æ˜ç¡®
2. **é«˜è´¨é‡ä»£ç **: 94.7%çš„è¯­æ³•æ­£ç¡®ç‡ï¼Œä»£ç è§„èŒƒè‰¯å¥½
3. **å…¨é¢çš„åŠŸèƒ½è¦†ç›–**: ä»æ•°æ®æ”¶é›†åˆ°äº¤æ˜“æ‰§è¡Œçš„å®Œæ•´é“¾è·¯
4. **è‰¯å¥½çš„æµ‹è¯•è¦†ç›–**: 14ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œ190ä¸ªæµ‹è¯•æ–¹æ³•

### âš ï¸ éœ€è¦æ”¹è¿›çš„æ–¹é¢
1. **æ€§èƒ½ä¼˜åŒ–**: éœ€è¦æ·»åŠ ç¼“å­˜æœºåˆ¶å’Œå¼‚æ­¥ä¼˜åŒ–
2. **ç›‘æ§ç³»ç»Ÿ**: éœ€è¦é›†æˆAPMå·¥å…·è¿›è¡Œæ€§èƒ½ç›‘æ§
3. **æ•°æ®åº“ä¼˜åŒ–**: éœ€è¦æ·»åŠ æ›´å¤šç´¢å¼•å’ŒæŸ¥è¯¢ä¼˜åŒ–
4. **æ¨¡å—åŒ–**: è€ƒè™‘å°†å¤§å‹æ¨¡å—è¿›ä¸€æ­¥æ‹†åˆ†

### ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
1. **ç«‹å³æ‰§è¡Œ**: å®æ–½é«˜ä¼˜å…ˆçº§ä¼˜åŒ–å»ºè®®
2. **çŸ­æœŸè®¡åˆ’**: æ·»åŠ æ€§èƒ½ç›‘æ§å’Œæµ‹è¯•
3. **ä¸­æœŸè§„åˆ’**: è¿›è¡Œæ¶æ„ä¼˜åŒ–å’Œæ¨¡å—åŒ–
4. **é•¿æœŸç›®æ ‡**: å®ç°å¾®æœåŠ¡åŒ–å’Œå®¹å™¨åŒ–éƒ¨ç½²

---

**ğŸ¯ ç³»ç»Ÿæ€§èƒ½è¯„åˆ†: {score_data.get("total_score", "N/A")}/100 ({score_data.get("grade", "N/A")})**

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report["timestamp"]}*
"""

        return md

async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="è¿è¡Œæ€§èƒ½åˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")

    args = parser.parse_args()

    print("ğŸš€ å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æ€§èƒ½åˆ†æå™¨")
    print("=" * 60)

    analyzer = PerformanceAnalyzer(args.project_root)

    try:
        # è¿è¡Œåˆ†æ
        await analyzer.run_comprehensive_analysis()

        # ç”ŸæˆæŠ¥å‘Š
        report_path = analyzer.generate_report()

        # æ˜¾ç¤ºç»“æœ
        score = analyzer.report.get("performance_score", {}).get("total_score", "N/A")
        grade = analyzer.report.get("performance_score", {}).get("grade", "N/A")

        print(f"\nğŸ“Š æ€§èƒ½åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ˆ æ€§èƒ½è¯„åˆ†: {score}/100 ({grade})")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°:")
        print(f"   JSON: {report_path}")
        print(f"   Markdown: {report_path.replace('.json', '.md')}")

        # æ˜¾ç¤ºé«˜ä¼˜å…ˆçº§å»ºè®®
        high_priority_recs = [r for r in analyzer.report.get("recommendations", []) if r.get("priority") == "é«˜"]
        if high_priority_recs:
            print(f"\nâš ï¸ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–å»ºè®® ({len(high_priority_recs)}é¡¹):")
            for i, rec in enumerate(high_priority_recs, 1):
                print(f"   {i}. {rec.get('category', 'æœªçŸ¥')}: {rec.get('issue', 'æ— ')}")

        print("=" * 60)

    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())