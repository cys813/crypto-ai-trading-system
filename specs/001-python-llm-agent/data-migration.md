# æ•°æ®è¿ç§»å’Œç‰ˆæœ¬æ§åˆ¶ç­–ç•¥

**ç‰ˆæœ¬**: 1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-10-08
**ç›®æ ‡**: ç¡®ä¿æ•°æ®åº“ç»“æ„æ¼”è¿›çš„å®‰å…¨æ€§å’Œä¸€è‡´æ€§

## ğŸ”„ æ•°æ®åº“ç‰ˆæœ¬æ§åˆ¶æ¶æ„

```mermaid
graph TB
    subgraph "å¼€å‘ç¯å¢ƒ"
        A[ä»£ç å˜æ›´] --> B[Alembicè¿ç§»ç”Ÿæˆ]
        B --> C[æœ¬åœ°æµ‹è¯•]
        C --> D[è¿ç§»è„šæœ¬æäº¤]
    end

    subgraph "æµ‹è¯•ç¯å¢ƒ"
        D --> E[è‡ªåŠ¨éƒ¨ç½²]
        E --> F[è¿ç§»æ‰§è¡Œ]
        F --> G[æ•°æ®éªŒè¯]
        G --> H[å›å½’æµ‹è¯•]
    end

    subgraph "ç”Ÿäº§ç¯å¢ƒ"
        H --> I[è“ç»¿éƒ¨ç½²]
        I --> J[è¿ç§»æ‰§è¡Œ]
        J --> K[å›æ»šå‡†å¤‡]
        K --> L[ç”Ÿäº§éªŒè¯]
    end

    subgraph "å¤‡ä»½ç­–ç•¥"
        M[è¿ç§»å‰å¤‡ä»½] --> N[å¢é‡å¤‡ä»½]
        N --> O[éªŒè¯å¤‡ä»½]
    end

    J --> M
```

## ğŸ“ Alembicé…ç½®

### 1. Alembicåˆå§‹åŒ–é…ç½®

```python
# alembic.ini
[alembic]
script_location = alembic
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = postgresql://crypto_trading:${DB_PASSWORD}@localhost:5432/crypto_trading_db

[post_write_hooks]
hooks = black
black.type = console_scripts
black.entrypoint = black
black.options = -l 79 REVISION_SCRIPT_FILENAME

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic
```

### 2. Alembicç¯å¢ƒé…ç½®

```python
# alembic/env.py
import asyncio
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from src.core.database import Base
from src.models import *  # å¯¼å…¥æ‰€æœ‰æ¨¡å‹
from src.core.config import settings

# ç›®æ ‡å…ƒæ•°æ®
target_metadata = Base.metadata

def run_migrations_offline():
    """ç¦»çº¿æ¨¡å¼è¿è¡Œè¿ç§»"""
    url = settings.DATABASE_URL
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection):
    """åœ¨çº¿æ¨¡å¼è¿è¡Œè¿ç§»"""
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True,
        compare_server_default=True,
        include_schemas=True,
    )

    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations():
    """å¼‚æ­¥è¿è¡Œè¿ç§»"""
    configuration = context.config
    configuration.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

    connectable = create_async_engine(
        settings.DATABASE_URL,
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

def run_migrations_online():
    """åœ¨çº¿æ¨¡å¼è¿è¡Œè¿ç§»"""
    asyncio.run(run_async_migrations())

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

## ğŸ“‹ è¿ç§»è„šæœ¬æ¨¡æ¿

### 1. åŸºç¡€è¡¨åˆ›å»ºè¿ç§»

```python
# alembic/versions/001_create_base_tables.py
"""Create base tables

Revision ID: 001
Revises:
Create Date: 2025-10-08 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    # åˆ›å»ºäº¤æ˜“ç¬¦å·è¡¨
    op.create_table('trading_symbols',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('symbol', sa.String(length=20), nullable=False),
        sa.Column('base_asset', sa.String(length=10), nullable=False),
        sa.Column('quote_asset', sa.String(length=10), nullable=False),
        sa.Column('status', sa.String(length=20), nullable=False),
        sa.Column('is_spot_trading', sa.Boolean(), nullable=True),
        sa.Column('is_margin_trading', sa.Boolean(), nullable=True),
        sa.Column('min_qty', sa.Numeric(precision=20, scale=8), nullable=True),
        sa.Column('max_qty', sa.Numeric(precision=20, scale=8), nullable=True),
        sa.Column('step_size', sa.Numeric(precision=20, scale=8), nullable=True),
        sa.Column('min_price', sa.Numeric(precision=20, scale=8), nullable=True),
        sa.Column('max_price', sa.Numeric(precision=20, scale=8), nullable=True),
        sa.Column('tick_size', sa.Numeric(precision=20, scale=8), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('symbol')
    )

    # åˆ›å»ºç´¢å¼•
    op.create_index(op.f('ix_trading_symbols_status'), 'trading_symbols', ['status'], unique=False)

    # åˆ›å»ºäº¤æ˜“æ‰€è¡¨
    op.create_table('exchanges',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('name', sa.String(length=100), nullable=False),
        sa.Column('code', sa.String(length=20), nullable=False),
        sa.Column('api_base_url', sa.String(length=500), nullable=False),
        sa.Column('api_version', sa.String(length=20), nullable=True),
        sa.Column('is_testnet', sa.Boolean(), nullable=True),
        sa.Column('rate_limit_requests_per_minute', sa.Integer(), nullable=True),
        sa.Column('rate_limit_orders_per_second', sa.Integer(), nullable=True),
        sa.Column('rate_limit_weight_per_minute', sa.Integer(), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=True),
        sa.Column('last_heartbeat', sa.DateTime(timezone=True), nullable=True),
        sa.Column('status', sa.String(length=20), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('code')
    )

    # åˆ›å»ºå…¶ä»–è¡¨...
    # [å®Œæ•´çš„è¡¨åˆ›å»ºä»£ç ]

def downgrade():
    op.drop_table('exchanges')
    op.drop_index(op.f('ix_trading_symbols_status'), table_name='trading_symbols')
    op.drop_table('trading_symbols')
```

### 2. TimescaleDBè¶…è¡¨åˆ›å»ºè¿ç§»

```python
# alembic/versions/002_create_timescaledb_tables.py
"""Create TimescaleDB hypertables

Revision ID: 002
Revises: 001
Create Date: 2025-10-08 11:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '002'
down_revision = '001'
branch_labels = None
depends_on = None

def upgrade():
    # åˆ›å»ºKçº¿æ•°æ®è¡¨
    op.execute("""
        CREATE TABLE kline_data (
            time TIMESTAMP WITH TIME ZONE NOT NULL,
            symbol_id UUID NOT NULL REFERENCES trading_symbols(id),
            exchange_id UUID NOT NULL REFERENCES exchanges(id),
            open_price DECIMAL(20,8) NOT NULL,
            high_price DECIMAL(20,8) NOT NULL,
            low_price DECIMAL(20,8) NOT NULL,
            close_price DECIMAL(20,8) NOT NULL,
            volume DECIMAL(20,8) NOT NULL,
            quote_volume DECIMAL(20,8),
            sma_20 DECIMAL(20,8),
            ema_12 DECIMAL(20,8),
            ema_26 DECIMAL(20,8),
            rsi DECIMAL(5,2),
            macd DECIMAL(20,8),
            macd_signal DECIMAL(20,8),
            bollinger_upper DECIMAL(20,8),
            bollinger_lower DECIMAL(20,8),
            PRIMARY KEY (time, symbol_id, exchange_id)
        );
    """)

    # è½¬æ¢ä¸ºTimescaleDBè¶…è¡¨
    op.execute("""
        SELECT create_hypertable('kline_data', 'time',
                               chunk_time_interval => INTERVAL '1 day');
    """)

    # åˆ›å»ºç´¢å¼•
    op.execute("""
        CREATE INDEX idx_kline_symbol_time ON kline_data (symbol_id, time DESC);
    """)

    op.execute("""
        CREATE INDEX idx_kline_exchange_time ON kline_data (exchange_id, time DESC);
    """)

    # åˆ›å»ºè¿ç»­èšåˆè§†å›¾
    op.execute("""
        CREATE MATERIALIZED VIEW kline_1hour
        WITH (timescaledb.continuous) AS
        SELECT
            time_bucket('1 hour', time) AS hour,
            symbol_id,
            exchange_id,
            first(open_price, time) AS open,
            max(high_price) AS high,
            min(low_price) AS low,
            last(close_price, time) AS close,
            sum(volume) AS volume
        FROM kline_data
        GROUP BY hour, symbol_id, exchange_id;
    """)

def downgrade():
    op.execute("DROP MATERIALIZED VIEW IF EXISTS kline_1hour;")
    op.execute("DROP TABLE IF EXISTS kline_data;")
```

## ğŸ”„ æ•°æ®è¿ç§»ç­–ç•¥

### 1. è¿ç§»ç±»å‹åˆ†ç±»

```python
# migrations/migration_types.py
from enum import Enum
from typing import Dict, List, Optional
from dataclasses import dataclass

class MigrationType(Enum):
    SCHEMA = "schema"           # ç»“æ„å˜æ›´
    DATA = "data"               # æ•°æ®è¿ç§»
    INDEX = "index"             # ç´¢å¼•å˜æ›´
    CONSTRAINT = "constraint"   # çº¦æŸå˜æ›´
    VIEW = "view"               # è§†å›¾å˜æ›´
    FUNCTION = "function"       # å‡½æ•°å˜æ›´

class MigrationRisk(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class MigrationPlan:
    migration_type: MigrationType
    risk_level: MigrationRisk
    estimated_downtime: int  # ç§’
    backup_required: bool
    rollback_possible: bool
    validation_queries: List[str]
    dependencies: List[str]
    notification_channels: List[str]
```

### 2. è‡ªåŠ¨åŒ–è¿ç§»ç®¡ç†å™¨

```python
# services/migration_manager.py
import asyncio
import subprocess
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import asyncpg
import redis

logger = logging.getLogger(__name__)

class MigrationManager:
    def __init__(self, db_pool, redis_client):
        self.db_pool = db_pool
        self.redis = redis_client

    async def execute_migration_plan(self, migration_id: str) -> bool:
        """æ‰§è¡Œè¿ç§»è®¡åˆ’"""
        try:
            # 1. è¿ç§»å‰æ£€æŸ¥
            if not await self._pre_migration_checks(migration_id):
                return False

            # 2. åˆ›å»ºå¤‡ä»½
            backup_path = await self._create_backup(migration_id)
            if not backup_path:
                return False

            # 3. æ‰§è¡Œè¿ç§»
            success = await self._execute_migration(migration_id)

            # 4. è¿ç§»åéªŒè¯
            if success:
                validation_result = await self._post_migration_validation(migration_id)
                if not validation_result:
                    await self._rollback_migration(migration_id, backup_path)
                    return False

            # 5. æ¸…ç†å’Œé€šçŸ¥
            await self._post_migration_cleanup(migration_id, success)
            return success

        except Exception as e:
            logger.error(f"Migration {migration_id} failed: {e}")
            await self._send_alert(f"Migration {migration_id} failed: {str(e)}")
            return False

    async def _pre_migration_checks(self, migration_id: str) -> bool:
        """è¿ç§»å‰æ£€æŸ¥"""
        checks = [
            self._check_database_connection,
            self._check_active_connections,
            self._check_disk_space,
            self._check_dependencies,
            self._check_migration_validity
        ]

        for check in checks:
            if not await check(migration_id):
                return False

        return True

    async def _check_database_connection(self, migration_id: str) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            async with self.db_pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            logger.info("Database connection check passed")
            return True
        except Exception as e:
            logger.error(f"Database connection check failed: {e}")
            return False

    async def _check_active_connections(self, migration_id: str) -> bool:
        """æ£€æŸ¥æ´»è·ƒè¿æ¥"""
        try:
            async with self.db_pool.acquire() as conn:
                active_connections = await conn.fetchval(
                    "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
                )

                if active_connections > 10:
                    logger.warning(f"High number of active connections: {active_connections}")
                    # å¯ä»¥é€‰æ‹©ç­‰å¾…æˆ–æ‹’ç»è¿ç§»

            logger.info("Active connections check passed")
            return True
        except Exception as e:
            logger.error(f"Active connections check failed: {e}")
            return False

    async def _create_backup(self, migration_id: str) -> Optional[str]:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = f"/backup/pre_migration_{migration_id}_{timestamp}.sql"

        try:
            # ä½¿ç”¨pg_dumpåˆ›å»ºå¤‡ä»½
            cmd = [
                "pg_dump",
                "-h", "localhost",
                "-U", "crypto_trading",
                "-d", "crypto_trading_db",
                "-f", backup_path,
                "--verbose",
                "--no-password"
            ]

            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.error(f"Backup failed: {stderr.decode()}")
                return None

            logger.info(f"Backup created successfully: {backup_path}")
            return backup_path

        except Exception as e:
            logger.error(f"Backup creation failed: {e}")
            return None

    async def _execute_migration(self, migration_id: str) -> bool:
        """æ‰§è¡Œè¿ç§»"""
        try:
            # æ‰§è¡ŒAlembicè¿ç§»
            cmd = ["alembic", "upgrade", "head"]

            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd="/app"
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.error(f"Migration execution failed: {stderr.decode()}")
                return False

            logger.info(f"Migration {migration_id} executed successfully")
            return True

        except Exception as e:
            logger.error(f"Migration execution failed: {e}")
            return False

    async def _post_migration_validation(self, migration_id: str) -> bool:
        """è¿ç§»åéªŒè¯"""
        try:
            # åŸºç¡€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            validation_queries = [
                "SELECT COUNT(*) FROM trading_symbols",
                "SELECT COUNT(*) FROM exchanges",
                "SELECT COUNT(*) FROM kline_data WHERE time >= NOW() - INTERVAL '1 hour'"
            ]

            async with self.db_pool.acquire() as conn:
                for query in validation_queries:
                    try:
                        result = await conn.fetchval(query)
                        logger.info(f"Validation query '{query}' result: {result}")
                    except Exception as e:
                        logger.error(f"Validation query failed: {e}")
                        return False

            # TimescaleDBè¶…è¡¨æ£€æŸ¥
            if await self._check_hypertable_health():
                logger.info("Hypertable health check passed")
            else:
                logger.error("Hypertable health check failed")
                return False

            logger.info("Post-migration validation passed")
            return True

        except Exception as e:
            logger.error(f"Post-migration validation failed: {e}")
            return False

    async def _check_hypertable_health(self) -> bool:
        """æ£€æŸ¥TimescaleDBè¶…è¡¨å¥åº·çŠ¶æ€"""
        try:
            async with self.db_pool.acquire() as conn:
                # æ£€æŸ¥è¶…è¡¨æ•°é‡
                hypertable_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM timescaledb_information.hypertables"
                )

                # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                data_check = await conn.fetchval(
                    "SELECT COUNT(*) FROM kline_data WHERE time > NOW() - INTERVAL '1 day'"
                )

                logger.info(f"Hypertables: {hypertable_count}, Recent data: {data_check}")
                return True

        except Exception as e:
            logger.error(f"Hypertable health check failed: {e}")
            return False

    async def _rollback_migration(self, migration_id: str, backup_path: str):
        """å›æ»šè¿ç§»"""
        try:
            logger.warning(f"Rolling back migration {migration_id}")

            # æ¢å¤å¤‡ä»½
            cmd = [
                "psql",
                "-h", "localhost",
                "-U", "crypto_trading",
                "-d", "crypto_trading_db",
                "-f", backup_path
            ]

            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.error(f"Rollback failed: {stderr.decode()}")
            else:
                logger.info("Rollback completed successfully")

        except Exception as e:
            logger.error(f"Rollback failed: {e}")

    async def _post_migration_cleanup(self, migration_id: str, success: bool):
        """è¿ç§»åæ¸…ç†"""
        try:
            # æ›´æ–°è¿ç§»çŠ¶æ€
            await self.redis.set(f"migration_status:{migration_id}",
                               "success" if success else "failed",
                               ex=86400)  # 24å°æ—¶è¿‡æœŸ

            # å‘é€é€šçŸ¥
            status = "æˆåŠŸ" if success else "å¤±è´¥"
            await self._send_notification(f"è¿ç§» {migration_id} {status}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            # TODO: å®ç°ä¸´æ—¶æ–‡ä»¶æ¸…ç†

        except Exception as e:
            logger.error(f"Post-migration cleanup failed: {e}")

    async def _send_notification(self, message: str):
        """å‘é€é€šçŸ¥"""
        # TODO: å®ç°é‚®ä»¶ã€Slackç­‰é€šçŸ¥
        logger.info(f"Notification: {message}")
```

## ğŸš€ éƒ¨ç½²ç­–ç•¥

### 1. è“ç»¿éƒ¨ç½²

```python
# deployments/blue_green_deployment.py
import asyncio
from typing import Dict, List
import docker

class BlueGreenDeployment:
    def __init__(self):
        self.docker_client = docker.from_env()
        self.current_environment = "blue"  # or "green"

    async def deploy_with_migration(self, migration_id: str):
        """è“ç»¿éƒ¨ç½²ä¸æ•°æ®åº“è¿ç§»"""
        try:
            # 1. å‡†å¤‡æ–°ç¯å¢ƒ
            new_environment = "green" if self.current_environment == "blue" else "blue"
            await self._prepare_environment(new_environment)

            # 2. åœ¨æ–°ç¯å¢ƒæ‰§è¡Œè¿ç§»
            migration_success = await self._execute_migration_in_environment(
                new_environment, migration_id
            )

            if not migration_success:
                await self._cleanup_environment(new_environment)
                return False

            # 3. éªŒè¯æ–°ç¯å¢ƒ
            if not await self._validate_environment(new_environment):
                await self._rollback_migration_in_environment(new_environment, migration_id)
                await self._cleanup_environment(new_environment)
                return False

            # 4. åˆ‡æ¢æµé‡
            await self._switch_traffic(new_environment)

            # 5. æ¸…ç†æ—§ç¯å¢ƒ
            old_environment = self.current_environment
            self.current_environment = new_environment
            await self._cleanup_environment(old_environment)

            return True

        except Exception as e:
            logger.error(f"Blue-green deployment failed: {e}")
            return False

    async def _prepare_environment(self, environment: str):
        """å‡†å¤‡æ–°ç¯å¢ƒ"""
        # å¯åŠ¨æ–°ç¯å¢ƒçš„æœåŠ¡
        services = [f"{environment}-app", f"{environment}-worker", f"{environment}-db"]

        for service in services:
            try:
                container = self.docker_client.containers.get(service)
                if container.status != "running":
                    container.start()
                logger.info(f"Started {service}")
            except docker.errors.NotFound:
                logger.error(f"Container {service} not found")

    async def _execute_migration_in_environment(self, environment: str, migration_id: str):
        """åœ¨æŒ‡å®šç¯å¢ƒæ‰§è¡Œè¿ç§»"""
        try:
            # è·å–ç¯å¢ƒçš„æ•°æ®åº“è¿æ¥
            db_container = self.docker_client.containers.get(f"{environment}-db")

            # åœ¨æ•°æ®åº“å®¹å™¨ä¸­æ‰§è¡Œè¿ç§»
            exit_code, output = db_container.exec_run(
                f"alembic upgrade head",
                workdir="/app"
            )

            if exit_code != 0:
                logger.error(f"Migration failed in {environment}: {output}")
                return False

            logger.info(f"Migration successful in {environment}")
            return True

        except Exception as e:
            logger.error(f"Migration execution failed in {environment}: {e}")
            return False

    async def _validate_environment(self, environment: str):
        """éªŒè¯æ–°ç¯å¢ƒ"""
        try:
            # å¥åº·æ£€æŸ¥
            app_container = self.docker_client.containers.get(f"{environment}-app")

            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            exit_code, output = app_container.exec_run(
                "curl -f http://localhost:8000/v1/health"
            )

            if exit_code != 0:
                logger.error(f"Health check failed in {environment}")
                return False

            logger.info(f"Environment {environment} validation passed")
            return True

        except Exception as e:
            logger.error(f"Environment validation failed: {e}")
            return False

    async def _switch_traffic(self, new_environment: str):
        """åˆ‡æ¢æµé‡åˆ°æ–°ç¯å¢ƒ"""
        try:
            # æ›´æ–°è´Ÿè½½å‡è¡¡å™¨é…ç½®
            # TODO: å®ç°è´Ÿè½½å‡è¡¡å™¨é…ç½®æ›´æ–°
            logger.info(f"Traffic switched to {new_environment}")
        except Exception as e:
            logger.error(f"Traffic switch failed: {e}")
```

### 2. æ•°æ®è¿ç§»ç›‘æ§

```python
# monitoring/migration_monitor.py
import asyncio
import time
from datetime import datetime
from typing import Dict, List

class MigrationMonitor:
    def __init__(self):
        self.metrics = {}

    async def monitor_migration_progress(self, migration_id: str):
        """ç›‘æ§è¿ç§»è¿›åº¦"""
        start_time = time.time()

        while True:
            try:
                # æ£€æŸ¥è¿ç§»çŠ¶æ€
                status = await self._get_migration_status(migration_id)
                progress = await self._get_migration_progress(migration_id)

                # è®°å½•æŒ‡æ ‡
                self.metrics[datetime.now()] = {
                    "status": status,
                    "progress": progress,
                    "elapsed_time": time.time() - start_time
                }

                # å‘é€è¿›åº¦æ›´æ–°
                await self._send_progress_update(migration_id, status, progress)

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if status in ["completed", "failed", "rolled_back"]:
                    break

                await asyncio.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡

            except Exception as e:
                logger.error(f"Migration monitoring error: {e}")
                await asyncio.sleep(30)

    async def _get_migration_status(self, migration_id: str) -> str:
        """è·å–è¿ç§»çŠ¶æ€"""
        # TODO: å®ç°çŠ¶æ€æŸ¥è¯¢
        return "running"

    async def _get_migration_progress(self, migration_id: str) -> float:
        """è·å–è¿ç§»è¿›åº¦"""
        # TODO: å®ç°è¿›åº¦æŸ¥è¯¢
        return 0.0

    async def _send_progress_update(self, migration_id: str, status: str, progress: float):
        """å‘é€è¿›åº¦æ›´æ–°"""
        # TODO: å®ç°è¿›åº¦é€šçŸ¥
        logger.info(f"Migration {migration_id}: {status} ({progress:.1f}%)")
```

## ğŸ“Š è¿ç§»æœ€ä½³å®è·µ

### 1. è¿ç§»æ¸…å•

```markdown
## è¿ç§»å‰æ£€æŸ¥æ¸…å•

- [ ] æ•°æ®åº“å¤‡ä»½å·²å®Œæˆå¹¶éªŒè¯
- [ ] è¿ç§»è„šæœ¬åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯é€šè¿‡
- [ ] è¿ç§»æ—¶é—´çª—å£å·²ç¡®è®¤
- [ ] å›æ»šè®¡åˆ’å·²å‡†å¤‡
- [ ] ç›‘æ§ç³»ç»Ÿå·²é…ç½®
- [ ] é€šçŸ¥æ¸ é“å·²ç¡®è®¤
- [ ] ä¾èµ–ç³»ç»Ÿå·²é€šçŸ¥
- [ ] è¿ç§»æ–‡æ¡£å·²æ›´æ–°

## è¿ç§»æ‰§è¡Œæ¸…å•

- [ ] åœæ­¢åº”ç”¨å†™å…¥ï¼ˆå¦‚éœ€è¦ï¼‰
- [ ] æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
- [ ] æ‰§è¡Œè¿ç§»è„šæœ¬
- [ ] éªŒè¯æ•°æ®å®Œæ•´æ€§
- [ ] æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
- [ ] éªŒè¯åº”ç”¨åŠŸèƒ½
- [ ] æ›´æ–°ç›‘æ§ç³»ç»Ÿ
- [ ] é€šçŸ¥ç›¸å…³å›¢é˜Ÿ

## è¿ç§»åæ£€æŸ¥æ¸…å•

- [ ] æ‰€æœ‰æœåŠ¡è¿è¡Œæ­£å¸¸
- [ ] æ•°æ®è®¿é—®æ€§èƒ½æ­£å¸¸
- [ ] ç›‘æ§æŒ‡æ ‡æ­£å¸¸
- [ ] é”™è¯¯æ—¥å¿—æ— å¼‚å¸¸
- [ ] ç”¨æˆ·åé¦ˆæ­£å¸¸
- [ ] å¤‡ä»½ç­–ç•¥å·²æ›´æ–°
- [ ] æ–‡æ¡£å·²æ›´æ–°
```

### 2. é£é™©ç¼“è§£ç­–ç•¥

```python
# risk_mitigation.py
class RiskMitigation:
    def __init__(self):
        self.risk_checks = {
            "data_loss": self._check_data_loss_risk,
            "performance": self._check_performance_risk,
            "rollback": self._check_rollback_risk,
            "downtime": self._check_downtime_risk
        }

    async def assess_migration_risk(self, migration_plan: Dict) -> Dict:
        """è¯„ä¼°è¿ç§»é£é™©"""
        risk_assessment = {}

        for risk_type, check_func in self.risk_checks.items():
            risk_level = await check_func(migration_plan)
            risk_assessment[risk_type] = risk_level

        return risk_assessment

    async def _check_data_loss_risk(self, migration_plan: Dict) -> str:
        """æ£€æŸ¥æ•°æ®ä¸¢å¤±é£é™©"""
        if migration_plan.get("backup_required", False):
            return "low"
        return "high"

    async def _check_performance_risk(self, migration_plan: Dict) -> str:
        """æ£€æŸ¥æ€§èƒ½é£é™©"""
        if migration_plan.get("estimated_downtime", 0) > 300:
            return "high"
        elif migration_plan.get("estimated_downtime", 0) > 60:
            return "medium"
        return "low"

    async def _check_rollback_risk(self, migration_plan: Dict) -> str:
        """æ£€æŸ¥å›æ»šé£é™©"""
        if migration_plan.get("rollback_possible", True):
            return "low"
        return "high"

    async def _check_downtime_risk(self, migration_plan: Dict) -> str:
        """æ£€æŸ¥åœæœºé£é™©"""
        downtime = migration_plan.get("estimated_downtime", 0)
        if downtime == 0:
            return "low"
        elif downtime < 60:
            return "medium"
        return "high"
```

## ğŸ”„ ç‰ˆæœ¬æ§åˆ¶é›†æˆ

### 1. Gitå·¥ä½œæµé›†æˆ

```bash
#!/bin/bash
# scripts/migration-workflow.sh

# 1. åˆ›å»ºè¿ç§»åˆ†æ”¯
git checkout -b feature/migration-$1

# 2. ç”Ÿæˆè¿ç§»æ–‡ä»¶
alembic revision --autogenerate -m "$2"

# 3. æäº¤è¿ç§»æ–‡ä»¶
git add alembic/versions/
git commit -m "Add migration: $2"

# 4. æ¨é€åˆ†æ”¯
git push origin feature/migration-$1

# 5. åˆ›å»ºPR
echo "è¯·åˆ›å»ºPRå¹¶ç­‰å¾…ä»£ç å®¡æŸ¥"
```

### 2. CI/CDé›†æˆ

```yaml
# .github/workflows/migration.yml
name: Database Migration

on:
  pull_request:
    paths:
      - 'alembic/versions/**'

jobs:
  test-migration:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: timescale/timescaledb:16.3.2-pg16
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-asyncio

    - name: Run migration tests
      run: |
        pytest tests/test_migrations.py -v

    - name: Test migration
      env:
        DATABASE_URL: postgresql://postgres:test_password@localhost:5432/test_db
      run: |
        alembic upgrade head
        alembic downgrade -1
        alembic upgrade head
```

è¿™ä¸ªæ•°æ®è¿ç§»å’Œç‰ˆæœ¬æ§åˆ¶ç­–ç•¥ç¡®ä¿äº†æ•°æ®åº“ç»“æ„æ¼”è¿›çš„å®‰å…¨æ€§ã€ä¸€è‡´æ€§å’Œå¯è¿½æº¯æ€§ï¼Œä¸ºç³»ç»Ÿé•¿æœŸç¨³å®šè¿è¡Œæä¾›äº†åšå®ä¿éšœã€‚
TodoWrite
<arg_key>todos</arg_key>
<arg_value>[{"content": "å¢åŠ Dockerå®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ", "status": "completed", "activeForm": "å¢åŠ Dockerå®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ"}, {"content": "å®Œå–„æˆæœ¬ç›‘æ§å’Œé¢„è­¦æœºåˆ¶", "status": "completed", "activeForm": "å®Œå–„æˆæœ¬ç›‘æ§å’Œé¢„è­¦æœºåˆ¶"}, {"content": "å¢åŠ æ•°æ®è¿ç§»å’Œç‰ˆæœ¬æ§åˆ¶ç­–ç•¥", "status": "completed", "activeForm": "å¢åŠ æ•°æ®è¿ç§»å’Œç‰ˆæœ¬æ§åˆ¶ç­–ç•¥"}, {"content": "æ·»åŠ APIç‰ˆæœ¬æ§åˆ¶ç­–ç•¥", "status": "in_progress", "activeForm": "æ·»åŠ APIç‰ˆæœ¬æ§åˆ¶ç­–ç•¥"}, {"content": "å¢å¼ºä¸šåŠ¡å±‚é¢ç›‘æ§æŒ‡æ ‡", "status": "pending", "activeForm": "å¢å¼ºä¸šåŠ¡å±‚é¢ç›‘æ§æŒ‡æ ‡"}, {"content": "ä¼˜åŒ–å¿«é€Ÿå¼€å§‹æŒ‡å—", "status": "pending", "activeForm": "ä¼˜åŒ–å¿«é€Ÿå¼€å§‹æŒ‡å—"}]