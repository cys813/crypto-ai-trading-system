# Quickstart Guide: OpenAI Compatible API Configuration

**Date**: 2025-10-08
**Feature**: OpenAI Compatible API with User-Specified Configuration

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€é…ç½®

#### ä½¿ç”¨OpenAIå®˜æ–¹APIï¼ˆé»˜è®¤ï¼‰
```bash
# .env æ–‡ä»¶é…ç½®
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
# OPENAI_BASE_URL ä¼šè‡ªåŠ¨ä½¿ç”¨ https://api.openai.com/v1
```

#### ä½¿ç”¨è‡ªå®šä¹‰OpenAIå…¼å®¹API
```bash
# SiliconFlowç¤ºä¾‹
OPENAI_API_KEY=your_siliconflow_api_key
OPENAI_BASE_URL=https://api.siliconflow.cn/v1
OPENAI_MODEL=gpt-4

# DeepSeekç¤ºä¾‹
OPENAI_API_KEY=your_deepseek_api_key
OPENAI_BASE_URL=https://api.deepseek.com/v1
OPENAI_MODEL=deepseek-chat

# Azure OpenAIç¤ºä¾‹
OPENAI_API_KEY=your_azure_openai_key
OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
OPENAI_MODEL=gpt-4
```

### 2. å®Œæ•´é…ç½®ç¤ºä¾‹

```bash
# .env å®Œæ•´é…ç½®ç¤ºä¾‹
# åº”ç”¨åŸºç¡€é…ç½®
APP_NAME=Crypto AI Trading System
DEBUG=false
ENVIRONMENT=production
SECRET_KEY=your-super-secret-key-here

# OpenAI APIé…ç½®
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.siliconflow.cn/v1  # å¯é€‰ï¼Œè‡ªå®šä¹‰ç«¯ç‚¹
OPENAI_ORGANIZATION=org-your-org-id             # å¯é€‰ï¼Œç»„ç»‡ID
OPENAI_MODEL=gpt-4                              # é»˜è®¤æ¨¡å‹
OPENAI_MAX_TOKENS=4096                          # æœ€å¤§tokenæ•°
OPENAI_TEMPERATURE=0.1                          # æ¸©åº¦å‚æ•°
OPENAI_TIMEOUT=60                               # è¯·æ±‚è¶…æ—¶(ç§’)
OPENAI_MAX_RETRIES=3                            # æœ€å¤§é‡è¯•æ¬¡æ•°
```

### 3. ä»£ç ä½¿ç”¨ç¤ºä¾‹

#### åŸºç¡€LLMè°ƒç”¨ï¼ˆæ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç ï¼‰
```python
from src.core.llm_integration import get_llm_service

# è·å–LLMæœåŠ¡å®ä¾‹
llm_service = get_llm_service()

# æ–‡æœ¬ç”Ÿæˆï¼ˆè‡ªåŠ¨ä½¿ç”¨é…ç½®çš„ç«¯ç‚¹ï¼‰
response = await llm_service.generate_completion(
    prompt="åˆ†æå½“å‰åŠ å¯†è´§å¸å¸‚åœºè¶‹åŠ¿",
    model="gpt-4",
    temperature=0.1
)

print(response)
```

#### å¸¦å®Œæ•´å“åº”çš„è°ƒç”¨
```python
from src.core.llm_integration import get_llm_service, LLMRequest

llm_service = get_llm_service()

# åˆ›å»ºè¯·æ±‚
request = LLMRequest(
    prompt="ç”Ÿæˆäº¤æ˜“ç­–ç•¥å»ºè®®",
    model="gpt-4",
    temperature=0.1,
    max_tokens=2000,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åˆ†æå¸ˆ"
)

# è·å–å®Œæ•´å“åº”
response = await llm_service.generate_completion_with_response(request)

print(f"å†…å®¹: {response.content}")
print(f"æ¨¡å‹: {response.model}")
print(f"ä½¿ç”¨çš„ç«¯ç‚¹: {getattr(response, 'base_url', 'default')}")
print(f"Tokenä½¿ç”¨: {response.tokens_used}")
print(f"å“åº”æ—¶é—´: {response.response_time_ms}ms")
print(f"æˆæœ¬: ${response.cost_usd}")
```

### 4. è¿æ¥æµ‹è¯•

```python
from src.core.llm_integration import LLMProvider, get_llm_service

llm_service = get_llm_service()

# æµ‹è¯•OpenAIè¿æ¥
is_connected = await llm_service.test_connection(LLMProvider.OPENAI)
if is_connected:
    print("âœ… OpenAIè¿æ¥æˆåŠŸ")
else:
    print("âŒ OpenAIè¿æ¥å¤±è´¥")

# è·å–æä¾›å•†çŠ¶æ€
status = llm_service.get_provider_status()
print(f"æä¾›å•†çŠ¶æ€: {status}")
```

### 5. é”™è¯¯å¤„ç†

```python
from src.core.llm_integration import LLMServiceError, get_llm_service

llm_service = get_llm_service()

try:
    response = await llm_service.generate_completion(
        prompt="æµ‹è¯•æç¤º",
        model="gpt-4"
    )
except LLMServiceError as e:
    print(f"LLMæœåŠ¡é”™è¯¯: {e.message}")
    print(f"é”™è¯¯ä»£ç : {e.error_code}")

    # æ ¹æ®é”™è¯¯ä»£ç è¿›è¡Œå¤„ç†
    if e.error_code == "OPENAI_API_ERROR":
        print("APIè°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé…ç½®")
    elif e.error_code == "OPENAI_NOT_INSTALLED":
        print("è¯·å®‰è£…OpenAIåŒ…: pip install openai")
```

## é…ç½®éªŒè¯

### éªŒè¯é…ç½®æ–‡ä»¶
```python
from src.core.config import settings

# æ£€æŸ¥OpenAIé…ç½®
if not settings.OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY æœªé…ç½®")

if settings.OPENAI_BASE_URL:
    print(f"ä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹: {settings.OPENAI_BASE_URL}")
else:
    print("ä½¿ç”¨OpenAIå®˜æ–¹ç«¯ç‚¹")

print(f"é»˜è®¤æ¨¡å‹: {settings.OPENAI_MODEL}")
print(f"è¶…æ—¶è®¾ç½®: {settings.OPENAI_TIMEOUT}ç§’")
print(f"æœ€å¤§é‡è¯•: {settings.OPENAI_MAX_RETRIES}æ¬¡")
```

### ç¯å¢ƒå˜é‡æ£€æŸ¥è„šæœ¬
```python
# check_config.py
import os
from dotenv import load_dotenv

load_dotenv()

required_vars = ["OPENAI_API_KEY"]
optional_vars = [
    "OPENAI_BASE_URL",
    "OPENAI_ORGANIZATION",
    "OPENAI_MODEL",
    "OPENAI_MAX_TOKENS",
    "OPENAI_TEMPERATURE",
    "OPENAI_TIMEOUT",
    "OPENAI_MAX_RETRIES"
]

print("ğŸ” æ£€æŸ¥OpenAIé…ç½®...")

# æ£€æŸ¥å¿…éœ€å˜é‡
missing_required = []
for var in required_vars:
    value = os.getenv(var)
    if value:
        print(f"âœ… {var}: {'*' * len(value)}")  # éšè—APIå¯†é’¥
    else:
        print(f"âŒ {var}: æœªé…ç½®")
        missing_required.append(var)

# æ£€æŸ¥å¯é€‰å˜é‡
for var in optional_vars:
    value = os.getenv(var)
    if value:
        print(f"âœ… {var}: {value}")
    else:
        print(f"âšª {var}: ä½¿ç”¨é»˜è®¤å€¼")

if missing_required:
    print(f"\nâŒ é…ç½®å¤±è´¥: ç¼ºå°‘å¿…éœ€å˜é‡ {missing_required}")
else:
    print("\nâœ… é…ç½®æ£€æŸ¥é€šè¿‡")
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„APIæä¾›å•†ï¼Ÿ
A: åªéœ€ä¿®æ”¹`.env`æ–‡ä»¶ä¸­çš„`OPENAI_BASE_URL`å’Œ`OPENAI_API_KEY`ï¼Œç„¶åé‡å¯æœåŠ¡å³å¯ã€‚

### Q: é…ç½®äº†é”™è¯¯çš„base_urlæ€ä¹ˆåŠï¼Ÿ
A: ç³»ç»Ÿä¼šåœ¨é¦–æ¬¡APIè°ƒç”¨æ—¶æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¿æ¥å¤±è´¥çš„åŸå› ã€‚æ£€æŸ¥URLæ ¼å¼å’Œç½‘ç»œè¿æ¥ï¼Œç„¶åé‡å¯æœåŠ¡ã€‚

### Q: å¦‚ä½•éªŒè¯è‡ªå®šä¹‰ç«¯ç‚¹çš„å…¼å®¹æ€§ï¼Ÿ
A: ä½¿ç”¨è¿æ¥æµ‹è¯•åŠŸèƒ½ï¼š`await llm_service.test_connection(LLMProvider.OPENAI)`

### Q: æ”¯æŒå“ªäº›OpenAIå…¼å®¹çš„APIï¼Ÿ
A: æ”¯æŒä»»ä½•éµå¾ªOpenAI APIè§„èŒƒçš„ç«¯ç‚¹ï¼ŒåŒ…æ‹¬SiliconFlowã€DeepSeekã€Azure OpenAIç­‰ã€‚

### Q: å¦‚ä½•è°ƒè¯•APIè°ƒç”¨é—®é¢˜ï¼Ÿ
A: æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè®°å½•è¯¦ç»†çš„APIè°ƒç”¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨çš„ç«¯ç‚¹ã€è¯·æ±‚å‚æ•°å’Œå“åº”æ—¶é—´ã€‚

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### å®‰å…¨é…ç½®
```bash
# ç”Ÿäº§ç¯å¢ƒé…ç½®
OPENAI_API_KEY=${OPENAI_API_KEY}  # ä»ç¯å¢ƒå˜é‡è¯»å–
OPENAI_BASE_URL=${OPENAI_BASE_URL}  # ä»ç¯å¢ƒå˜é‡è¯»å–
OPENAI_TIMEOUT=30  # ç”Ÿäº§ç¯å¢ƒå»ºè®®è¾ƒçŸ­è¶…æ—¶
OPENAI_MAX_RETRIES=2  # ç”Ÿäº§ç¯å¢ƒå»ºè®®è¾ƒå°‘é‡è¯•
```

### ç›‘æ§é…ç½®
```python
# æ·»åŠ ç›‘æ§å’Œæ—¥å¿—
import logging

logger = logging.getLogger(__name__)

async def monitored_llm_call(prompt: str) -> str:
    llm_service = get_llm_service()

    start_time = time.time()
    try:
        response = await llm_service.generate_completion(prompt)
        duration = time.time() - start_time

        logger.info(
            "LLMè°ƒç”¨æˆåŠŸ",
            prompt_length=len(prompt),
            response_length=len(response),
            duration=duration
        )
        return response
    except Exception as e:
        duration = time.time() - start_time
        logger.error(
            "LLMè°ƒç”¨å¤±è´¥",
            error=str(e),
            duration=duration
        )
        raise
```

è¿™ä¸ªå¿«é€Ÿå¼€å§‹æŒ‡å—æ¶µç›–äº†ä»åŸºç¡€é…ç½®åˆ°ç”Ÿäº§éƒ¨ç½²çš„æ‰€æœ‰å…³é”®æ­¥éª¤ã€‚