#!/usr/bin/env python3
"""
æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨

ç”¨äºç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…æ‹¬æµ‹è¯•è¦†ç›–ç‡ã€æ€§èƒ½æŒ‡æ ‡å’Œé—®é¢˜åˆ†æã€‚
"""

import os
import sys
import json
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import argparse

class TestReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.backend_root = self.project_root / "backend"
        self.test_results = {}
        self.report_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "project_root": str(project_root),
                "python_version": sys.version,
                "platform": sys.platform
            },
            "test_results": {},
            "coverage": {},
            "performance": {},
            "issues": [],
            "summary": {}
        }

    def run_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶...")

        test_suites = [
            {
                "name": "Contract Tests",
                "path": "tests/contract",
                "pattern": "test_*.py",
                "description": "APIç«¯ç‚¹åˆçº¦æµ‹è¯•"
            },
            {
                "name": "Integration Tests",
                "path": "tests/integration",
                "pattern": "test_*.py",
                "description": "ç³»ç»Ÿé›†æˆæµ‹è¯•"
            },
            {
                "name": "Unit Tests",
                "path": "tests/unit",
                "pattern": "test_*.py",
                "description": "å•å…ƒæµ‹è¯•"
            }
        ]

        for suite in test_suites:
            print(f"\nğŸ“‹ è¿è¡Œ {suite['name']}...")
            result = self._run_test_suite(suite)
            self.report_data["test_results"][suite["name"]] = result

            # è¾“å‡ºæµ‹è¯•ç»“æœ
            self._print_test_result(suite["name"], result)

        return self.report_data["test_results"]

    def _run_test_suite(self, suite: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        test_path = self.backend_root / suite["path"]

        if not test_path.exists():
            return {
                "status": "skipped",
                "reason": f"æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_path}",
                "total": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "errors": [],
                "duration": 0,
                "test_files": []
            }

        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        test_files = list(test_path.glob(suite["pattern"]))

        if not test_files:
            return {
                "status": "skipped",
                "reason": f"æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶: {suite['pattern']}",
                "total": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "errors": [],
                "duration": 0,
                "test_files": []
            }

        # è¿è¡Œæµ‹è¯•
        return self._run_pytest(test_path, suite["name"], test_files)

    def _run_pytest(self, test_path: Path, suite_name: str, test_files: List[Path]) -> Dict[str, Any]:
        """ä½¿ç”¨pytestè¿è¡Œæµ‹è¯•"""
        start_time = time.time()

        try:
            # æ„å»ºpytestå‘½ä»¤
            cmd = [
                sys.executable, "-m", "pytest",
                str(test_path),
                "-v",
                "--tb=short",
                "--json-report",
                "--json-report-file=/tmp/pytest_report.json",
                "--html=/tmp/test_report.html",
                "--self-contained-html",
                "--cov=" + str(self.backend_root / "src"),
                "--cov-report=term-missing",
                "--cov-report=html:/tmp/coverage_html",
                "--cov-report=json:/tmp/coverage.json"
            ]

            # åˆ‡æ¢åˆ°backendç›®å½•
            original_cwd = os.getcwd()
            os.chdir(self.backend_root)

            # è¿è¡Œpytest
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )

            duration = time.time() - start_time

            # è§£æç»“æœ
            if result.returncode == 0:
                return self._parse_pytest_success(result, test_files, duration)
            else:
                return self._parse_pytest_failure(result, test_files, duration)

        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "reason": "æµ‹è¯•æ‰§è¡Œè¶…æ—¶",
                "total": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "errors": ["æµ‹è¯•æ‰§è¡Œè¶…æ—¶"],
                "duration": 300,
                "test_files": [f.name for f in test_files]
            }
        except Exception as e:
            return {
                "status": "error",
                "reason": f"æ‰§è¡Œé”™è¯¯: {str(e)}",
                "total": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "errors": [str(e)],
                "duration": time.time() - start_time,
                "test_files": [f.name for f in test_files]
            }
        finally:
            os.chdir(original_cwd)

    def _parse_pytest_success(self, result: subprocess.CompletedProcess, test_files: List[Path], duration: float) -> Dict[str, Any]:
        """è§£æpytestæˆåŠŸç»“æœ"""
        try:
            # å°è¯•è¯»å–JSONæŠ¥å‘Š
            with open("/tmp/pytest_report.json", "r") as f:
                json_report = json.load(f)

            summary = json_report.get("summary", {})

            return {
                "status": "passed",
                "total": summary.get("total", 0),
                "passed": summary.get("passed", 0),
                "failed": summary.get("failed", 0),
                "skipped": summary.get("skipped", 0),
                "errors": [],
                "duration": duration,
                "test_files": [f.name for f in test_files],
                "details": {
                    "summary": summary,
                    "collected": summary.get("collected", 0)
                }
            }
        except Exception as e:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»stdoutè§£æ
            return self._parse_pytest_output(result.stdout, test_files, duration, "passed")

    def _parse_pytest_failure(self, result: subprocess.CompletedProcess, test_files: List[Path], duration: float) -> Dict[str, Any]:
        """è§£æpytestå¤±è´¥ç»“æœ"""
        try:
            with open("/tmp/pytest_report.json", "r") as f:
                json_report = json.load(f)

            summary = json_report.get("summary", {})

            # æå–é”™è¯¯ä¿¡æ¯
            errors = []
            if "errors" in json_report:
                for error in json_report["errors"]:
                    errors.append(f"{error.get('test_id', 'Unknown')}: {error.get('message', 'Unknown error')}")

            # ä»è¾“å‡ºä¸­æå–é¢å¤–é”™è¯¯ä¿¡æ¯
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if "FAILED" in line and "test_" in line:
                    if line not in errors:
                        errors.append(line.strip())

            return {
                "status": "failed",
                "total": summary.get("total", 0),
                "passed": summary.get("passed", 0),
                "failed": summary.get("failed", 0),
                "skipped": summary.get("skipped", 0),
                "errors": errors[:10],  # é™åˆ¶é”™è¯¯æ•°é‡
                "duration": duration,
                "test_files": [f.name for f in test_files],
                "details": {
                    "summary": summary,
                    "collected": summary.get("collected", 0),
                    "returncode": result.returncode,
                    "stdout": result.stdout[-1000:],  # ä¿ç•™æœ€å1000å­—ç¬¦
                    "stderr": result.stderr[-1000:]
                }
            }
        except Exception as e:
            return self._parse_pytest_output(result.stdout, test_files, duration, "failed")

    def _parse_pytest_output(self, output: str, test_files: List[Path], duration: float, status: str) -> Dict[str, Any]:
        """ä»pytestè¾“å‡ºè§£æç»“æœ"""
        lines = output.split('\n')

        total = 0
        passed = 0
        failed = 0
        skipped = 0
        errors = []

        for line in lines:
            if "=" in line and "passed" in line.lower():
                # è§£æç±»ä¼¼ "2 passed, 1 failed in 5.23s" çš„è¡Œ
                parts = line.split()
                for part in parts:
                    if part.isdigit():
                        num = int(part)
                        if "passed" in line:
                            passed = num
                        elif "failed" in line:
                            failed = num
                        elif "skipped" in line:
                            skipped = num
                        total = max(total, num)
            elif "FAILED" in line:
                errors.append(line.strip())
            elif "ERROR" in line:
                errors.append(line.strip())

        return {
            "status": status,
            "total": total,
            "passed": passed,
            "failed": failed,
            "skipped": skipped,
            "errors": errors[:10],
            "duration": duration,
            "test_files": [f.name for f in test_files],
            "details": {
                "raw_output": output
            }
        }

    def _print_test_result(self, suite_name: str, result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        status_icon = "âœ…" if result["status"] == "passed" else "âŒ" if result["status"] == "failed" else "âš ï¸"

        print(f"{status_icon} {suite_name}")
        print(f"   æ€»è®¡: {result['total']}, é€šè¿‡: {result['passed']}, å¤±è´¥: {result['failed']}, è·³è¿‡: {result['skipped']}")
        print(f"   è€—æ—¶: {result['duration']:.2f}s")

        if result["status"] != "passed":
            print(f"   çŠ¶æ€: {result['status']}")
            if result.get("reason"):
                print(f"   åŸå› : {result['reason']}")
            if result["errors"]:
                print(f"   é”™è¯¯: {len(result['errors'])}ä¸ª")
                for error in result["errors"][:3]:  # æ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"     - {error}")
        print()

    def check_code_quality(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç è´¨é‡"""
        print("ğŸ” æ£€æŸ¥ä»£ç è´¨é‡...")

        quality_checks = {
            "flake8": self._run_flake8(),
            "isort": self._run_isort_check(),
            "black": self._run_black_check(),
            "mypy": self._run_mypy_check()
        }

        self.report_data["code_quality"] = quality_checks
        return quality_checks

    def _run_flake8(self) -> Dict[str, Any]:
        """è¿è¡Œflake8ä»£ç æ£€æŸ¥"""
        try:
            original_cwd = os.getcwd()
            os.chdir(self.backend_root)

            result = subprocess.run(
                [sys.executable, "-m", "flake8", "src/", "--count"],
                capture_output=True,
                text=True
            )

            os.chdir(original_cwd)

            return {
                "status": "passed" if result.returncode == 0 else "failed",
                "issues": int(result.stdout.strip()) if result.stdout.strip() else 0,
                "output": result.stdout
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _run_isort_check(self) -> Dict[str, Any]:
        """è¿è¡Œisortæ£€æŸ¥"""
        try:
            original_cwd = os.getcwd()
            os.chdir(self.backend_root)

            result = subprocess.run(
                [sys.executable, "-m", "isort", "src/", "--check-only"],
                capture_output=True,
                text=True
            )

            os.chdir(original_cwd)

            return {
                "status": "passed" if result.returncode == 0 else "failed",
                "output": result.stdout
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _run_black_check(self) -> Dict[str, Any]:
        """è¿è¡Œblackæ ¼å¼æ£€æŸ¥"""
        try:
            original_cwd = os.getcwd()
            os.chdir(self.backend_root)

            result = subprocess.run(
                [sys.executable, "-m", "black", "src/", "--check"],
                capture_output=True,
                text=True
            )

            os.chdir(original_cwd)

            return {
                "status": "passed" if result.returncode == 0 else "failed",
                "output": result.stdout
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _run_mypy_check(self) -> Dict[str, Any]:
        """è¿è¡Œmypyç±»å‹æ£€æŸ¥"""
        try:
            original_cwd = os.getcwd()
            os.chdir(self.backend_root)

            result = subprocess.run(
                [sys.executable, "-m", "mypy", "src/", "--ignore-missing-imports"],
                capture_output=True,
                text=True,
                timeout=60
            )

            os.chdir(original_cwd)

            return {
                "status": "passed" if result.returncode == 0 else "failed",
                "output": result.stdout
            }
        except subprocess.TimeoutExpired:
            return {"status": "timeout", "error": "ç±»å‹æ£€æŸ¥è¶…æ—¶"}
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def analyze_code_complexity(self) -> Dict[str, Any]:
        """åˆ†æä»£ç å¤æ‚åº¦"""
        print("ğŸ“Š åˆ†æä»£ç å¤æ‚åº¦...")

        complexity_metrics = {
            "total_files": 0,
            "total_lines": 0,
            "python_files": 0,
            "test_files": 0,
            "service_files": 0,
            "model_files": 0,
            "largest_files": [],
            "complexity_analysis": {}
        }

        # æ‰«æPythonæ–‡ä»¶
        for root, dirs, files in os.walk(self.backend_root):
            # è·³è¿‡ç‰¹å®šç›®å½•
            dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.pytest_cache']]

            for file in files:
                if file.endswith('.py'):
                    file_path = Path(root) / file
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            line_count = len(lines)

                            complexity_metrics["python_files"] += 1
                            complexity_metrics["total_lines"] += line_count
                            complexity_metrics["total_files"] += 1

                            # åˆ†ç±»æ–‡ä»¶
                            if "test" in file_path.parts:
                                complexity_metrics["test_files"] += 1
                            elif "services" in file_path.parts:
                                complexity_metrics["service_files"] += 1
                            elif "models" in file_path.parts:
                                complexity_metrics["model_files"] += 1

                            # è®°å½•æœ€å¤§æ–‡ä»¶
                            complexity_metrics["largest_files"].append({
                                "path": str(file_path.relative_to(self.backend_root)),
                                "lines": line_count
                            })

                    except Exception as e:
                        print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")

        # æ’åºæœ€å¤§çš„æ–‡ä»¶
        complexity_metrics["largest_files"] = sorted(
            complexity_metrics["largest_files"],
            key=lambda x: x["lines"],
            reverse=True
        )[:10]

        self.report_data["complexity"] = complexity_metrics
        return complexity_metrics

    def generate_report(self, output_file: str = "test_report.json"):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        total_tests = sum(r.get("total", 0) for r in self.report_data["test_results"].values())
        total_passed = sum(r.get("passed", 0) for r in self.report_data["test_results"].values())
        total_failed = sum(r.get("failed", 0) for r in self.report_data["test_results"].values())
        total_skipped = sum(r.get("skipped", 0) for r in self.report_data["test_results"].values())

        self.report_data["summary"] = {
            "total_tests": total_tests,
            "total_passed": total_passed,
            "total_failed": total_failed,
            "total_skipped": total_skipped,
            "success_rate": (total_passed / total_tests * 100) if total_tests > 0 else 0,
            "test_suites": len(self.report_data["test_results"]),
            "all_passed": total_failed == 0
        }

        # ä¿å­˜JSONæŠ¥å‘Š
        report_path = self.project_root / output_file
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, indent=2, ensure_ascii=False, default=str)

        print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report()

        return report_path

    def _generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        md_content = self._create_markdown_content()

        md_path = self.project_root / "test_report.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_path}")

    def _create_markdown_content(self) -> str:
        """åˆ›å»ºMarkdownæŠ¥å‘Šå†…å®¹"""
        summary = self.report_data["summary"]
        complexity = self.report_data.get("complexity", {})

        md = f"""# å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æµ‹è¯•æŠ¥å‘Š

## ğŸ“Š æµ‹è¯•æ±‡æ€»

- **æµ‹è¯•æ—¶é—´**: {self.report_data["metadata"]["generated_at"]}
- **æ€»æµ‹è¯•æ•°**: {summary["total_tests"]}
- **é€šè¿‡æµ‹è¯•**: {summary["total_passed"]}
- **å¤±è´¥æµ‹è¯•**: {summary["total_failed"]}
- **è·³è¿‡æµ‹è¯•**: {summary["total_skipped"]}
- **æˆåŠŸç‡**: {summary["success_rate"]:.1f}%
- **æµ‹è¯•å¥—ä»¶**: {summary["test_suites"]}
- **æ•´ä½“çŠ¶æ€**: {"âœ… å…¨éƒ¨é€šè¿‡" if summary["all_passed"] else "âŒ å­˜åœ¨å¤±è´¥"}

## ğŸ§ª æµ‹è¯•ç»“æœè¯¦æƒ…

"""

        # æµ‹è¯•å¥—ä»¶è¯¦æƒ…
        for suite_name, result in self.report_data["test_results"].items():
            status_icon = "âœ…" if result["status"] == "passed" else "âŒ" if result["status"] == "failed" else "âš ï¸"
            md += f"""
### {status_icon} {suite_name}

- **çŠ¶æ€**: {result["status"]}
- **æ€»è®¡**: {result["total"]}
- **é€šè¿‡**: {result["passed"]}
- **å¤±è´¥**: {result["failed"]}
- **è·³è¿‡**: {result["skipped"]}
- **è€—æ—¶**: {result["duration"]:.2f}s
- **æµ‹è¯•æ–‡ä»¶**: {len(result.get("test_files", []))}

"""

            if result["errors"]:
                md += "#### é”™è¯¯è¯¦æƒ…:\n"
                for error in result["errors"]:
                    md += f"- `{error}`\n"
                md += "\n"

        # ä»£ç è´¨é‡
        if "code_quality" in self.report_data:
            md += "## ğŸ” ä»£ç è´¨é‡æ£€æŸ¥\n\n"
            quality = self.report_data["code_quality"]

            for check_name, result in quality.items():
                status_icon = "âœ…" if result["status"] == "passed" else "âŒ" if result["status"] == "failed" else "âš ï¸"
                md += f"### {status_icon} {check_name.upper()}\n"
                md += f"- **çŠ¶æ€**: {result['status']}\n"

                if check_name == "flake8":
                    md += f"- **é—®é¢˜æ•°**: {result.get('issues', 0)}\n"

                if "output" in result and result["output"]:
                    output_lines = result["output"].split('\n')[:5]
                    for line in output_lines:
                        if line.strip():
                            md += f"- {line.strip()}\n"
                md += "\n"

        # ä»£ç å¤æ‚åº¦
        if complexity:
            md += "## ğŸ“Š ä»£ç å¤æ‚åº¦åˆ†æ\n\n"
            md += f"- **Pythonæ–‡ä»¶æ€»æ•°**: {complexity.get('python_files', 0)}\n"
            md += f"- **ä»£ç æ€»è¡Œæ•°**: {complexity.get('total_lines', 0)}\n"
            md += f"- **æµ‹è¯•æ–‡ä»¶æ•°**: {complexity.get('test_files', 0)}\n"
            md += f"- **æœåŠ¡æ–‡ä»¶æ•°**: {complexity.get('service_files', 0)}\n"
            md += f"- **æ¨¡å‹æ–‡ä»¶æ•°**: {complexity.get('model_files', 0)}\n"

            if complexity.get("largest_files"):
                md += "\n#### æœ€å¤§æ–‡ä»¶ (æŒ‰è¡Œæ•°):\n"
                for file_info in complexity["largest_files"][:5]:
                    md += f"- `{file_info['path']}`: {file_info['lines']} è¡Œ\n"
            md += "\n"

        # ç³»ç»Ÿä¿¡æ¯
        md += "## â„¹ï¸ ç³»ç»Ÿä¿¡æ¯\n\n"
        metadata = self.report_data["metadata"]
        md += f"- **Pythonç‰ˆæœ¬**: {metadata['python_version']}\n"
        md += f"- **å¹³å°**: {metadata['platform']}\n"
        md += f"- **é¡¹ç›®è·¯å¾„**: {metadata['project_root']}\n"

        md += """
## ğŸ“ ç»“è®º

"""

        if summary["all_passed"]:
            md += """âœ… **æµ‹è¯•ç»“æœ**: æ‰€æœ‰æµ‹è¯•å‡é€šè¿‡ï¼Œç³»ç»ŸåŠŸèƒ½æ­£å¸¸

ğŸ‰ **ç³»ç»ŸçŠ¶æ€**: æ ¸å¿ƒåŠŸèƒ½å·²å®Œæ•´å®ç°ï¼Œå…·å¤‡ç”Ÿäº§éƒ¨ç½²æ¡ä»¶

"""
        else:
            md += f"""âŒ **æµ‹è¯•ç»“æœ**: å­˜åœ¨ {summary['total_failed']} ä¸ªå¤±è´¥æµ‹è¯•

âš ï¸ **å»ºè®®**: éœ€è¦ä¿®å¤å¤±è´¥çš„æµ‹è¯•åæ‰èƒ½éƒ¨ç½²

"""

        md += "---\n"
        md += f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {self.report_data['metadata']['generated_at']}*\n"

        return md

    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        summary = self.report_data["summary"]

        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯•æ€»ç»“")
        print("="*60)
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"é€šè¿‡: {summary['total_passed']}")
        print(f"å¤±è´¥: {summary['total_failed']}")
        print(f"è·³è¿‡: {summary['total_skipped']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print("="*60)

        if summary["all_passed"]:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½éƒ¨ç½²ã€‚")
        else:
            print("âš ï¸ å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œè¯·æ£€æŸ¥å¹¶ä¿®å¤é—®é¢˜ã€‚")

        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡Œæµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--output", default="test_report.json", help="æµ‹è¯•æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--skip-quality", action="store_true", help="è·³è¿‡ä»£ç è´¨é‡æ£€æŸ¥")
    parser.add_argument("--skip-complexity", action="store_true", help="è·³è¿‡ä»£ç å¤æ‚åº¦åˆ†æ")

    args = parser.parse_args()

    print("ğŸš€ å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨")
    print("="*60)

    generator = TestReportGenerator(args.project_root)

    try:
        # è¿è¡Œæµ‹è¯•
        generator.run_tests()

        # ä»£ç è´¨é‡æ£€æŸ¥
        if not args.skip_quality:
            generator.check_code_quality()

        # ä»£ç å¤æ‚åº¦åˆ†æ
        if not args.skip_complexity:
            generator.analyze_code_complexity()

        # ç”ŸæˆæŠ¥å‘Š
        generator.generate_report(args.output)

        # æ‰“å°æ€»ç»“
        generator.print_summary()

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()