#!/usr/bin/env python3
"""
APIæ€§èƒ½åˆ†æè„šæœ¬
åˆ†æAPIå“åº”æ—¶é—´ã€å¹¶å‘å¤„ç†èƒ½åŠ›å’Œæ€§èƒ½ç“¶é¢ˆ
"""

import os
import re
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple

class APIPerformanceAnalyzer:
    """APIæ€§èƒ½åˆ†æå™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.backend_root = self.project_root / "backend"
        self.report = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(project_root),
            "analysis": {},
            "recommendations": [],
            "performance_score": {}
        }

    def analyze_api_structure(self):
        """åˆ†æAPIç»“æ„"""
        print("ğŸŒ åˆ†æAPIç»“æ„...")

        api_analysis = {
            "endpoints": [],
            "middleware": [],
            "async_endpoints": 0,
            "sync_endpoints": 0,
            "response_models": 0,
            "validation_complexity": {},
            "error_handling": 0
        }

        try:
            api_dir = self.backend_root / "src" / "api"
            if not api_dir.exists():
                print("æœªæ‰¾åˆ°apiç›®å½•")
                return

            api_files = list(api_dir.rglob("*.py"))

            for api_file in api_files:
                if api_file.name == "__init__.py":
                    continue

                file_analysis = self._analyze_api_file(api_file)
                if file_analysis:
                    api_analysis["endpoints"].extend(file_analysis["endpoints"])
                    api_analysis["middleware"].extend(file_analysis["middleware"])
                    api_analysis["async_endpoints"] += file_analysis["async_endpoints"]
                    api_analysis["sync_endpoints"] += file_analysis["sync_endpoints"]
                    api_analysis["response_models"] += file_analysis["response_models"]
                    api_analysis["error_handling"] += file_analysis["error_handling"]

                    # åˆå¹¶éªŒè¯å¤æ‚åº¦
                    for key, value in file_analysis["validation_complexity"].items():
                        if key in api_analysis["validation_complexity"]:
                            api_analysis["validation_complexity"][key] += value
                        else:
                            api_analysis["validation_complexity"][key] = value

        except Exception as e:
            print(f"APIç»“æ„åˆ†æå¤±è´¥: {e}")

        self.report["analysis"]["api_structure"] = api_analysis

    def _analyze_api_file(self, api_file: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªAPIæ–‡ä»¶"""
        try:
            with open(api_file, 'r', encoding='utf-8') as f:
                content = f.read()

            file_analysis = {
                "endpoints": [],
                "middleware": [],
                "async_endpoints": 0,
                "sync_endpoints": 0,
                "response_models": 0,
                "validation_complexity": {},
                "error_handling": 0
            }

            # æŸ¥æ‰¾APIç«¯ç‚¹
            endpoint_patterns = [
                r'@(?:app|router)\.(get|post|put|delete|patch)\(["\']([^"\']+)["\']',
                r'@.*\.route\(["\']([^"\']+)["\'].*?,\s*methods=\[([^\]]+)\]'
            ]

            for pattern in endpoint_patterns:
                matches = re.findall(pattern, content, re.MULTILINE)
                for match in matches:
                    if len(match) == 2:
                        method, path = match
                    else:
                        path, methods = match
                        method = methods.strip(' "').split(',')[0] if ',' in methods else 'GET'

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚æ­¥
                    async_pattern = rf'async\s+def\s+\w+.*?(?:@.*?(?:get|post|put|delete|patch).*?["\']{re.escape(path)}["\'])'
                    is_async = bool(re.search(async_pattern, content, re.DOTALL))

                    if is_async:
                        file_analysis["async_endpoints"] += 1
                    else:
                        file_analysis["sync_endpoints"] += 1

                    file_analysis["endpoints"].append({
                        "method": method.upper(),
                        "path": path,
                        "file": str(api_file.relative_to(self.backend_root)),
                        "is_async": is_async
                    })

            # æŸ¥æ‰¾ä¸­é—´ä»¶
            middleware_patterns = [
                r'@(?:app|router)\.middleware\("([^"]+)"\)',
                r'@.*\.before_request',
                r'@.*\.after_request'
            ]

            for pattern in middleware_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    file_analysis["middleware"].append({
                        "type": match,
                        "file": str(api_file.relative_to(self.backend_root))
                    })

            # ç»Ÿè®¡å“åº”æ¨¡å‹
            file_analysis["response_models"] = content.count('response_model') + content.count('BaseModel')

            # ç»Ÿè®¡éªŒè¯å¤æ‚åº¦
            validation_keywords = ['Field(', 'validator(', 'Pydantic', 'BaseModel', 'Query(', 'Path(', 'Body(']
            for keyword in validation_keywords:
                count = content.count(keyword)
                if count > 0:
                    file_analysis["validation_complexity"][keyword] = count

            # ç»Ÿè®¡é”™è¯¯å¤„ç†
            file_analysis["error_handling"] = content.count('HTTPException') + content.count('raise ') + content.count('try:')

            return file_analysis

        except Exception as e:
            print(f"åˆ†æAPIæ–‡ä»¶å¤±è´¥ {api_file}: {e}")
            return None

    def analyze_concurrency_patterns(self):
        """åˆ†æå¹¶å‘æ¨¡å¼"""
        print("âš¡ åˆ†æå¹¶å‘æ¨¡å¼...")

        concurrency_analysis = {
            "async_functions": 0,
            "await_usage": 0,
            "asyncio_usage": 0,
            "threading_usage": 0,
            "pool_usage": 0,
            "blocking_operations": 0,
            "concurrency_patterns": []
        }

        try:
            python_files = list(self.backend_root.rglob("*.py"))

            for py_file in python_files[:30]:  # é‡‡æ ·åˆ†æ
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # ç»Ÿè®¡å¼‚æ­¥ç›¸å…³
                    concurrency_analysis["async_functions"] += content.count('async def')
                    concurrency_analysis["await_usage"] += content.count('await ')
                    concurrency_analysis["asyncio_usage"] += content.count('asyncio.')

                    # ç»Ÿè®¡çº¿ç¨‹ç›¸å…³
                    concurrency_analysis["threading_usage"] += content.count('threading.') + content.count('Thread(')

                    # ç»Ÿè®¡è¿æ¥æ± 
                    concurrency_analysis["pool_usage"] += content.count('pool') + content.count('Pool(')

                    # ç»Ÿè®¡é˜»å¡æ“ä½œ
                    blocking_patterns = [
                        r'time\.sleep\(',
                        r'requests\.',
                        r'session\.execute\(',
                        r'file\.read\(',
                        r'subprocess\.'
                    ]

                    for pattern in blocking_patterns:
                        matches = re.findall(pattern, content)
                        concurrency_analysis["blocking_operations"] += len(matches)

                    # åˆ†æå¹¶å‘æ¨¡å¼
                    if 'async def' in content or 'await ' in content:
                        concurrency_analysis["concurrency_patterns"].append({
                            "file": str(py_file.relative_to(self.backend_root)),
                            "type": "async",
                            "async_functions": content.count('async def'),
                            "await_usage": content.count('await ')
                        })

                except:
                    continue

        except Exception as e:
            print(f"å¹¶å‘æ¨¡å¼åˆ†æå¤±è´¥: {e}")

        self.report["analysis"]["concurrency"] = concurrency_analysis

    def analyze_response_optimization(self):
        """åˆ†æå“åº”ä¼˜åŒ–"""
        print("ğŸš€ åˆ†æå“åº”ä¼˜åŒ–...")

        response_analysis = {
            "compression": 0,
            "streaming": 0,
            "caching": 0,
            "batching": 0,
            "pagination": 0,
            "optimization_features": [],
            "performance_patterns": []
        }

        try:
            api_files = list(self.backend_root.rglob("*.py"))

            for api_file in api_files[:20]:  # é‡‡æ ·åˆ†æ
                try:
                    with open(api_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # ç»Ÿè®¡ä¼˜åŒ–ç‰¹æ€§
                    if 'gzip' in content.lower() or 'compression' in content.lower():
                        response_analysis["compression"] += 1

                    if 'streaming' in content.lower() or 'StreamingResponse' in content:
                        response_analysis["streaming"] += 1

                    if 'cache' in content.lower():
                        response_analysis["caching"] += 1

                    if 'batch' in content.lower():
                        response_analysis["batching"] += 1

                    if 'pagination' in content.lower() or 'page' in content.lower():
                        response_analysis["pagination"] += 1

                    # åˆ†ææ€§èƒ½æ¨¡å¼
                    patterns = self._extract_performance_patterns(content)
                    if patterns:
                        response_analysis["performance_patterns"].append({
                            "file": str(api_file.relative_to(self.backend_root)),
                            "patterns": patterns
                        })

                except:
                    continue

        except Exception as e:
            print(f"å“åº”ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")

        self.report["analysis"]["response_optimization"] = response_analysis

    def _extract_performance_patterns(self, content: str) -> List[str]:
        """æå–æ€§èƒ½æ¨¡å¼"""
        patterns = []

        performance_keywords = [
            'background task',
            'celery',
            'async',
            'cache',
            'compress',
            'stream',
            'batch',
            'lazy loading',
            'eager loading',
            'connection pool',
            'rate limit'
        ]

        for keyword in performance_keywords:
            if keyword in content.lower():
                patterns.append(keyword)

        return patterns

    def analyze_security_performance(self):
        """åˆ†æå®‰å…¨æ€§èƒ½"""
        print("ğŸ”’ åˆ†æå®‰å…¨æ€§èƒ½...")

        security_analysis = {
            "rate_limiting": 0,
            "authentication": 0,
            "authorization": 0,
            "input_validation": 0,
            "cors_config": 0,
            "security_headers": 0,
            "security_overhead": []
        }

        try:
            api_files = list(self.backend_root.rglob("*.py"))

            for api_file in api_files:
                try:
                    with open(api_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # ç»Ÿè®¡å®‰å…¨ç‰¹æ€§
                    if 'rate' in content.lower() and 'limit' in content.lower():
                        security_analysis["rate_limiting"] += 1

                    if 'auth' in content.lower() or 'oauth' in content.lower():
                        security_analysis["authentication"] += 1

                    if 'permission' in content.lower() or 'role' in content.lower():
                        security_analysis["authorization"] += 1

                    if 'validate' in content.lower() or 'validation' in content.lower():
                        security_analysis["input_validation"] += 1

                    if 'cors' in content.lower():
                        security_analysis["cors_config"] += 1

                    # åˆ†æå®‰å…¨å¼€é”€
                    security_patterns = self._extract_security_overhead(content)
                    if security_patterns:
                        security_analysis["security_overhead"].append({
                            "file": str(api_file.relative_to(self.backend_root)),
                            "patterns": security_patterns
                        })

                except:
                    continue

        except Exception as e:
            print(f"å®‰å…¨æ€§èƒ½åˆ†æå¤±è´¥: {e}")

        self.report["analysis"]["security_performance"] = security_analysis

    def _extract_security_overhead(self, content: str) -> List[str]:
        """æå–å®‰å…¨å¼€é”€"""
        overhead_patterns = []

        security_overhead_keywords = [
            'hashing',
            'encryption',
            'jwt',
            'token validation',
            'rate limiting',
            'input sanitization',
            'sql injection prevention',
            'xss protection'
        ]

        for keyword in security_overhead_keywords:
            if keyword in content.lower():
                overhead_patterns.append(keyword)

        return overhead_patterns

    def generate_performance_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        print("ğŸ’¡ ç”ŸæˆAPIæ€§èƒ½å»ºè®®...")

        recommendations = []

        # åŸºäºAPIç»“æ„çš„å»ºè®®
        api_structure = self.report["analysis"].get("api_structure", {})
        total_endpoints = len(api_structure.get("endpoints", []))
        async_endpoints = api_structure.get("async_endpoints", 0)
        sync_endpoints = api_structure.get("sync_endpoints", 0)

        if sync_endpoints > async_endpoints:
            recommendations.append({
                "category": "å¼‚æ­¥ä¼˜åŒ–",
                "priority": "é«˜",
                "issue": f"åŒæ­¥ç«¯ç‚¹è¿‡å¤š ({sync_endpoints} vs {async_endpoints})",
                "suggestion": "å°†I/Oå¯†é›†å‹APIç«¯ç‚¹è½¬æ¢ä¸ºå¼‚æ­¥å®ç°",
                "impact": "æå‡å¹¶å‘å¤„ç†èƒ½åŠ› 50-100%"
            })

        # åŸºäºå¹¶å‘æ¨¡å¼çš„å»ºè®®
        concurrency = self.report["analysis"].get("concurrency", {})
        blocking_ops = concurrency.get("blocking_operations", 0)
        async_functions = concurrency.get("async_functions", 0)

        if blocking_ops > async_functions * 2:
            recommendations.append({
                "category": "å¹¶å‘ä¼˜åŒ–",
                "priority": "é«˜",
                "issue": f"é˜»å¡æ“ä½œè¿‡å¤š ({blocking_ops} ä¸ª)",
                "suggestion": "ä½¿ç”¨å¼‚æ­¥æ“ä½œæ›¿æ¢é˜»å¡è°ƒç”¨ï¼Œå®æ–½è¿æ¥æ± ",
                "impact": "å‡å°‘å“åº”æ—¶é—´ 30-50%"
            })

        # åŸºäºå“åº”ä¼˜åŒ–çš„å»ºè®®
        response_opt = self.report["analysis"].get("response_optimization", {})
        compression = response_opt.get("compression", 0)
        caching = response_opt.get("caching", 0)

        if compression == 0:
            recommendations.append({
                "category": "å“åº”å‹ç¼©",
                "priority": "ä¸­",
                "issue": "ç¼ºå°‘å“åº”å‹ç¼©",
                "suggestion": "å¯ç”¨Gzipå‹ç¼©å‡å°‘ä¼ è¾“æ•°æ®é‡",
                "impact": "å‡å°‘ä¼ è¾“æ—¶é—´ 40-60%"
            })

        if caching < total_endpoints * 0.3:
            recommendations.append({
                "category": "å“åº”ç¼“å­˜",
                "priority": "é«˜",
                "issue": "ç¼“å­˜è¦†ç›–ç‡ä¸è¶³",
                "suggestion": "ä¸ºé¢‘ç¹è®¿é—®çš„APIç«¯ç‚¹æ·»åŠ ç¼“å­˜å±‚",
                "impact": "æå‡å“åº”é€Ÿåº¦ 60-80%"
            })

        # åŸºäºå®‰å…¨æ€§èƒ½çš„å»ºè®®
        security = self.report["analysis"].get("security_performance", {})
        rate_limiting = security.get("rate_limiting", 0)

        if rate_limiting == 0:
            recommendations.append({
                "category": "é™æµä¿æŠ¤",
                "priority": "é«˜",
                "issue": "ç¼ºå°‘APIé™æµæœºåˆ¶",
                "suggestion": "å®æ–½åŸºäºIPå’Œç”¨æˆ·çš„è¯·æ±‚é™æµ",
                "impact": "é˜²æ­¢æ»¥ç”¨ï¼Œä¿æŠ¤ç³»ç»Ÿç¨³å®šæ€§"
            })

        # é€šç”¨ä¼˜åŒ–å»ºè®®
        recommendations.extend([
            {
                "category": "ç›‘æ§å’Œæ—¥å¿—",
                "priority": "ä¸­",
                "issue": "ç¼ºå°‘æ€§èƒ½ç›‘æ§",
                "suggestion": "é›†æˆAPMå·¥å…·ç›‘æ§APIå“åº”æ—¶é—´å’Œé”™è¯¯ç‡",
                "impact": "åŠæ—¶å‘ç°æ€§èƒ½é—®é¢˜"
            },
            {
                "category": "è´Ÿè½½å‡è¡¡",
                "priority": "ä½",
                "issue": "å•ç‚¹éƒ¨ç½²é£é™©",
                "suggestion": "é…ç½®è´Ÿè½½å‡è¡¡å™¨å®ç°æ°´å¹³æ‰©å±•",
                "impact": "æé«˜ç³»ç»Ÿå¯ç”¨æ€§å’Œå¤„ç†èƒ½åŠ›"
            },
            {
                "category": "CDNä¼˜åŒ–",
                "priority": "ä½",
                "issue": "é™æ€èµ„æºåŠ è½½æ…¢",
                "suggestion": "ä½¿ç”¨CDNåŠ é€Ÿé™æ€èµ„æºè®¿é—®",
                "impact": "æå‡é™æ€èµ„æºè®¿é—®é€Ÿåº¦"
            }
        ])

        self.report["recommendations"] = recommendations

    def calculate_performance_score(self):
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        print("ğŸ“ˆ è®¡ç®—APIæ€§èƒ½è¯„åˆ†...")

        scores = {
            "async_performance": 70,
            "response_optimization": 65,
            "concurrency_handling": 75,
            "security_performance": 80
        }

        # åŸºäºåˆ†æç»“æœè°ƒæ•´åˆ†æ•°
        api_structure = self.report["analysis"].get("api_structure", {})
        concurrency = self.report["analysis"].get("concurrency", {})
        response_opt = self.report["analysis"].get("response_optimization", {})
        security = self.report["analysis"].get("security_performance", {})

        # å¼‚æ­¥æ€§èƒ½è¯„åˆ†
        total_endpoints = len(api_structure.get("endpoints", []))
        if total_endpoints > 0:
            async_ratio = api_structure.get("async_endpoints", 0) / total_endpoints
            if async_ratio > 0.8:
                scores["async_performance"] = 90
            elif async_ratio > 0.5:
                scores["async_performance"] = 80
            elif async_ratio > 0.3:
                scores["async_performance"] = 70
            else:
                scores["async_performance"] = 50

        # å“åº”ä¼˜åŒ–è¯„åˆ†
        optimization_features = sum([
            response_opt.get("compression", 0),
            response_opt.get("caching", 0),
            response_opt.get("streaming", 0),
            response_opt.get("pagination", 0)
        ])

        if optimization_features > 3:
            scores["response_optimization"] = 85
        elif optimization_features > 1:
            scores["response_optimization"] = 75
        else:
            scores["response_optimization"] = 60

        # å¹¶å‘å¤„ç†è¯„åˆ†
        async_functions = concurrency.get("async_functions", 0)
        blocking_ops = concurrency.get("blocking_operations", 0)

        if async_functions > blocking_ops:
            scores["concurrency_handling"] = 85
        elif async_functions > blocking_ops * 0.5:
            scores["concurrency_handling"] = 75
        else:
            scores["concurrency_handling"] = 65

        # å®‰å…¨æ€§èƒ½è¯„åˆ†
        security_features = sum([
            security.get("rate_limiting", 0),
            security.get("authentication", 0),
            security.get("authorization", 0),
            security.get("input_validation", 0)
        ])

        if security_features > 3:
            scores["security_performance"] = 90
        elif security_features > 2:
            scores["security_performance"] = 80
        else:
            scores["security_performance"] = 70

        # è®¡ç®—æ€»åˆ†
        total_score = sum(scores.values()) / len(scores)

        self.report["performance_score"] = {
            "total_score": round(total_score, 1),
            "grade": self._get_grade(total_score),
            "individual_scores": scores
        }

    def _get_grade(self, score: float) -> str:
        """è·å–ç­‰çº§"""
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (ä¸€èˆ¬)"
        elif score >= 60:
            return "C (éœ€è¦æ”¹è¿›)"
        else:
            return "D (æ€¥éœ€ä¼˜åŒ–)"

    def generate_report(self):
        """ç”ŸæˆAPIæ€§èƒ½æŠ¥å‘Š"""
        print("ğŸ“ ç”ŸæˆAPIæ€§èƒ½æŠ¥å‘Š...")

        # ç”ŸæˆJSONæŠ¥å‘Š
        json_path = self.project_root / "api_performance_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_path = self.project_root / "api_performance_report.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(self._create_markdown_report())

        return str(json_path)

    def _create_markdown_report(self) -> str:
        """åˆ›å»ºMarkdownæŠ¥å‘Š"""
        report = self.report
        score = report.get("performance_score", {})

        md = f"""# å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - APIæ€§èƒ½åˆ†ææŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚è§ˆ

**åˆ†ææ—¶é—´**: {report["timestamp"]}
**é¡¹ç›®è·¯å¾„**: {report["project_root"]}

### ğŸ¯ æ€§èƒ½è¯„åˆ†
**æ€»åˆ†**: {score.get("total_score", "N/A")}/100 ({score.get("grade", "N/A")})

| ç»´åº¦ | å¾—åˆ† | è¯„ä»· |
|------|------|------|
| å¼‚æ­¥æ€§èƒ½ | {score.get("individual_scores", {}).get("async_performance", "N/A")} | å¼‚æ­¥å¤„ç†èƒ½åŠ› |
| å“åº”ä¼˜åŒ– | {score.get("individual_scores", {}).get("response_optimization", "N/A")} | å“åº”æ—¶é—´å’Œå‹ç¼© |
| å¹¶å‘å¤„ç† | {score.get("individual_scores", {}).get("concurrency_handling", "N/A")} | å¹¶å‘å¤„ç†èƒ½åŠ› |
| å®‰å…¨æ€§èƒ½ | {score.get("individual_scores", {}).get("security_performance", "N/A")} | å®‰å…¨æœºåˆ¶å¼€é”€ |

---

## ğŸŒ APIç»“æ„åˆ†æ

### ğŸ“Š ç«¯ç‚¹ç»Ÿè®¡
"""

        api_structure = report["analysis"].get("api_structure", {})
        total_endpoints = len(api_structure.get("endpoints", []))
        async_endpoints = api_structure.get("async_endpoints", 0)
        sync_endpoints = api_structure.get("sync_endpoints", 0)

        md += f"- **æ€»ç«¯ç‚¹æ•°**: {total_endpoints}\n"
        md += f"- **å¼‚æ­¥ç«¯ç‚¹**: {async_endpoints} ({async_endpoints/total_endpoints*100:.1f}%)\n"
        md += f"- **åŒæ­¥ç«¯ç‚¹**: {sync_endpoints} ({sync_endpoints/total_endpoints*100:.1f}%)\n"
        md += f"- **ä¸­é—´ä»¶æ•°**: {len(api_structure.get('middleware', []))}\n"
        md += f"- **å“åº”æ¨¡å‹**: {api_structure.get('response_models', 0)}\n"

        md += f"""
### ğŸ“‹ ç«¯ç‚¹åˆ†å¸ƒ
"""

        # æŒ‰æ–¹æ³•åˆ†ç±»ç»Ÿè®¡
        method_counts = {}
        for endpoint in api_structure.get("endpoints", []):
            method = endpoint.get("method", "UNKNOWN")
            method_counts[method] = method_counts.get(method, 0) + 1

        for method, count in sorted(method_counts.items()):
            md += f"- **{method}**: {count} ä¸ªç«¯ç‚¹\n"

        md += f"""
### ğŸ”§ éªŒè¯å¤æ‚åº¦
"""

        validation_complexity = api_structure.get("validation_complexity", {})
        for key, count in sorted(validation_complexity.items(), key=lambda x: x[1], reverse=True):
            md += f"- **{key}**: {count} æ¬¡ä½¿ç”¨\n"

        md += f"""

---

## âš¡ å¹¶å‘æ¨¡å¼åˆ†æ

### ğŸ“Š å¹¶å‘ç»Ÿè®¡
"""

        concurrency = report["analysis"].get("concurrency", {})
        md += f"- **å¼‚æ­¥å‡½æ•°**: {concurrency.get('async_functions', 0)} ä¸ª\n"
        md += f"- **Awaitä½¿ç”¨**: {concurrency.get('await_usage', 0)} æ¬¡\n"
        md += f"- **AsyncIOä½¿ç”¨**: {concurrency.get('asyncio_usage', 0)} æ¬¡\n"
        md += f"- **çº¿ç¨‹ä½¿ç”¨**: {concurrency.get('threading_usage', 0)} æ¬¡\n"
        md += f"- **è¿æ¥æ± **: {concurrency.get('pool_usage', 0)} ä¸ª\n"
        md += f"- **é˜»å¡æ“ä½œ**: {concurrency.get('blocking_operations', 0)} ä¸ª\n"

        md += f"""
### ğŸ“‹ å¹¶å‘æ¨¡å¼
"""

        concurrency_patterns = concurrency.get("concurrency_patterns", [])
        for pattern in concurrency_patterns[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            md += f"- `{pattern['file']}`: {pattern['async_functions']} å¼‚æ­¥å‡½æ•°, {pattern['await_usage']} await\n"

        md += f"""

---

## ğŸš€ å“åº”ä¼˜åŒ–åˆ†æ

### ğŸ“Š ä¼˜åŒ–ç‰¹æ€§ç»Ÿè®¡
"""

        response_opt = report["analysis"].get("response_optimization", {})
        md += f"- **å“åº”å‹ç¼©**: {response_opt.get('compression', 0)} ä¸ªå®ç°\n"
        md += f"- **æµå¼å“åº”**: {response_opt.get('streaming', 0)} ä¸ªå®ç°\n"
        md += f"- **å“åº”ç¼“å­˜**: {response_opt.get('caching', 0)} ä¸ªå®ç°\n"
        md += f"- **æ‰¹å¤„ç†**: {response_opt.get('batching', 0)} ä¸ªå®ç°\n"
        md += f"- **åˆ†é¡µ**: {response_opt.get('pagination', 0)} ä¸ªå®ç°\n"

        md += f"""
### ğŸ“‹ æ€§èƒ½æ¨¡å¼
"""

        performance_patterns = response_opt.get("performance_patterns", [])
        for pattern in performance_patterns[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            md += f"- `{pattern['file']}`: {', '.join(pattern['patterns'])}\n"

        md += f"""

---

## ğŸ”’ å®‰å…¨æ€§èƒ½åˆ†æ

### ğŸ“Š å®‰å…¨ç‰¹æ€§ç»Ÿè®¡
"""

        security = report["analysis"].get("security_performance", {})
        md += f"- **é™æµä¿æŠ¤**: {security.get('rate_limiting', 0)} ä¸ªå®ç°\n"
        md += f"- **èº«ä»½è®¤è¯**: {security.get('authentication', 0)} ä¸ªå®ç°\n"
        md += f"- **æƒé™æ§åˆ¶**: {security.get('authorization', 0)} ä¸ªå®ç°\n"
        md += f"- **è¾“å…¥éªŒè¯**: {security.get('input_validation', 0)} ä¸ªå®ç°\n"
        md += f"- **CORSé…ç½®**: {security.get('cors_config', 0)} ä¸ªå®ç°\n"

        md += f"""
### ğŸ“‹ å®‰å…¨å¼€é”€
"""

        security_overhead = security.get("security_overhead", [])
        for overhead in security_overhead[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            md += f"- `{overhead['file']}`: {', '.join(overhead['patterns'])}\n"

        md += f"""

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ğŸ¯ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        recommendations = report.get("recommendations", [])
        high_priority = [r for r in recommendations if r.get("priority") == "é«˜"]
        for rec in high_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **é¢„æœŸå½±å“**: {rec.get("impact", "æ— ")}
"""

        md += """
### ğŸ“‹ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        medium_priority = [r for r in recommendations if r.get("priority") == "ä¸­"]
        for rec in medium_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **é¢„æœŸå½±å“**: {rec.get("impact", "æ— ")}
"""

        md += """
### ğŸ“ ä½ä¼˜å…ˆçº§ä¼˜åŒ–
"""

        low_priority = [r for r in recommendations if r.get("priority") == "ä½"]
        for rec in low_priority:
            md += f"""
#### {rec.get("category", "æœªçŸ¥")}
- **é—®é¢˜**: {rec.get("issue", "æ— ")}
- **å»ºè®®**: {rec.get("suggestion", "æ— ")}
- **é¢„æœŸå½±å“**: {rec.get("impact", "æ— ")}
"""

        md += f"""

---

## ğŸš€ å®æ–½è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µ (ç«‹å³æ‰§è¡Œ - 1å‘¨å†…)
1. **å¼‚æ­¥åŒ–æ”¹é€ **: å°†åŒæ­¥APIç«¯ç‚¹è½¬æ¢ä¸ºå¼‚æ­¥
2. **å“åº”å‹ç¼©**: å¯ç”¨Gzipå‹ç¼©
3. **åŸºç¡€ç¼“å­˜**: ä¸ºå…³é”®APIæ·»åŠ ç¼“å­˜

### ç¬¬äºŒé˜¶æ®µ (çŸ­æœŸä¼˜åŒ– - 2-4å‘¨)
1. **å¹¶å‘ä¼˜åŒ–**: ä¼˜åŒ–é˜»å¡æ“ä½œï¼Œå®æ–½è¿æ¥æ± 
2. **é™æµä¿æŠ¤**: å®ç°APIé™æµæœºåˆ¶
3. **æ€§èƒ½ç›‘æ§**: é›†æˆAPMç›‘æ§å·¥å…·

### ç¬¬ä¸‰é˜¶æ®µ (é•¿æœŸä¼˜åŒ– - 1-3ä¸ªæœˆ)
1. **é«˜çº§ç¼“å­˜**: å®æ–½å¤šçº§ç¼“å­˜ç­–ç•¥
2. **è´Ÿè½½å‡è¡¡**: é…ç½®è´Ÿè½½åˆ†å‘
3. **CDNé›†æˆ**: ä¼˜åŒ–é™æ€èµ„æºè®¿é—®

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡

### ğŸ¯ å…³é”®æŒ‡æ ‡æ”¹è¿›
- **å“åº”æ—¶é—´**: å‡å°‘ 40-70%
- **å¹¶å‘å¤„ç†èƒ½åŠ›**: æå‡ 50-150%
- **ååé‡**: æå‡ 60-120%
- **èµ„æºåˆ©ç”¨ç‡**: æå‡ 30-50%

### ğŸ’° ä¸šåŠ¡ä»·å€¼
- **ç”¨æˆ·ä½“éªŒ**: æ˜¾è‘—æå‡å“åº”é€Ÿåº¦
- **ç³»ç»Ÿç¨³å®šæ€§**: å¢å¼ºé«˜è´Ÿè½½å¤„ç†èƒ½åŠ›
- **è¿è¥æˆæœ¬**: æé«˜èµ„æºä½¿ç”¨æ•ˆç‡
- **æ‰©å±•èƒ½åŠ›**: æ”¯æŒæ›´å¤§è§„æ¨¡è®¿é—®

---

## ğŸ” ç›‘æ§å»ºè®®

### ï¿½ï¿½ å…³é”®ç›‘æ§æŒ‡æ ‡
1. **å“åº”æ—¶é—´**: å¹³å‡ã€P95ã€P99å“åº”æ—¶é—´
2. **ååé‡**: æ¯ç§’è¯·æ±‚æ•°(QPS)
3. **é”™è¯¯ç‡**: 4xxã€5xxé”™è¯¯æ¯”ä¾‹
4. **å¹¶å‘æ•°**: åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°
5. **èµ„æºä½¿ç”¨**: CPUã€å†…å­˜ã€ç½‘ç»œä½¿ç”¨ç‡

### ğŸš¨ å‘Šè­¦é˜ˆå€¼å»ºè®®
- **å“åº”æ—¶é—´**: P95 > 500ms
- **é”™è¯¯ç‡**: > 1%
- **å¹¶å‘æ•°**: > 80% æœ€å¤§å®¹é‡
- **CPUä½¿ç”¨ç‡**: > 80%
- **å†…å­˜ä½¿ç”¨ç‡**: > 85%

---

**ğŸ¯ APIæ€§èƒ½è¯„åˆ†: {score.get("total_score", "N/A")}/100 ({score.get("grade", "N/A")})**

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report["timestamp"]}*
"""

        return md

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="è¿è¡ŒAPIæ€§èƒ½åˆ†æ")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")

    args = parser.parse_args()

    print("ğŸš€ å¤šAgentåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - APIæ€§èƒ½åˆ†æå™¨")
    print("=" * 60)

    analyzer = APIPerformanceAnalyzer(args.project_root)

    try:
        # è¿è¡Œåˆ†æ
        analyzer.analyze_api_structure()
        analyzer.analyze_concurrency_patterns()
        analyzer.analyze_response_optimization()
        analyzer.analyze_security_performance()
        analyzer.generate_performance_recommendations()
        analyzer.calculate_performance_score()

        # ç”ŸæˆæŠ¥å‘Š
        report_path = analyzer.generate_report()

        # æ˜¾ç¤ºç»“æœ
        score = analyzer.report.get("performance_score", {}).get("total_score", "N/A")
        grade = analyzer.report.get("performance_score", {}).get("grade", "N/A")

        print(f"\nğŸ“Š APIæ€§èƒ½åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ˆ æ€§èƒ½è¯„åˆ†: {score}/100 ({grade})")
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°:")
        print(f"   JSON: {report_path}")
        print(f"   Markdown: {report_path.replace('.json', '.md')}")

        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        api_structure = analyzer.report["analysis"].get("api_structure", {})
        concurrency = analyzer.report["analysis"].get("concurrency", {})

        print(f"\nğŸ“Š å…³é”®ç»Ÿè®¡:")
        print(f"   APIç«¯ç‚¹: {len(api_structure.get('endpoints', []))} ä¸ª")
        print(f"   å¼‚æ­¥ç«¯ç‚¹: {api_structure.get('async_endpoints', 0)} ä¸ª")
        print(f"   å¼‚æ­¥å‡½æ•°: {concurrency.get('async_functions', 0)} ä¸ª")
        print(f"   é˜»å¡æ“ä½œ: {concurrency.get('blocking_operations', 0)} ä¸ª")

        print("=" * 60)

    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()